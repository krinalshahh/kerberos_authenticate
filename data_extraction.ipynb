{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-27T09:06:38.300280Z",
     "start_time": "2025-05-27T09:03:13.419131Z"
    }
   },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "from collections import deque\n",
    "\n",
    "class News_crawler:\n",
    "    def __init__(self, start_url, max_articles=20, max_depth=5, delay=1.5):\n",
    "        self.start_url = start_url\n",
    "        self.max_articles = max_articles\n",
    "        self.max_depth = max_depth\n",
    "        self.delay = delay\n",
    "        self.visited_urls = set()\n",
    "        self.articles = []\n",
    "        self.ua = UserAgent()\n",
    "        self.session = requests.Session()\n",
    "        self.domain = urlparse(start_url).netloc\n",
    "        self.queue = deque()\n",
    "\n",
    "        #session headers\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': self.ua.random,\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Referer': 'https://www.google.com/',\n",
    "            'DNT': '1',\n",
    "        })\n",
    "\n",
    "    def is_valid_url(self, url):\n",
    "        parsed = urlparse(url)\n",
    "        return (parsed.netloc == self.domain and url not in self.visited_urls and not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png']))\n",
    "\n",
    "    def is_article_page(self, url, soup):\n",
    "        article = soup.find('article')\n",
    "        headline = soup.find('h1')\n",
    "        date_published = soup.find('time') or soup.find('meta', property='article:published_time')\n",
    "        return (article is not None or headline is not None or date_published is not None or '/article/' in url or any(seg in url for seg in ['/news/', '/story/', '/post/']))\n",
    "\n",
    "    def extract_article_content(self, soup):\n",
    "        article = (soup.find('article') or soup.find('div', class_=lambda x: x and 'article' in x.lower()) or soup.find('div', id=lambda x: x and 'content' in x.lower()) or soup.find('main'))\n",
    "        if not article:\n",
    "            return None\n",
    "        for element in article.find_all(['script', 'style', 'nav', 'footer', 'aside', 'figure', 'img']):\n",
    "            element.decompose()\n",
    "        return article.get_text(separator='\\n', strip=True)\n",
    "\n",
    "    def process_page(self, url, depth):\n",
    "        try:\n",
    "            time.sleep(self.delay)\n",
    "            response = self.session.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            self.visited_urls.add(url)\n",
    "\n",
    "            #check for article page\n",
    "            if self.is_article_page(url, soup):\n",
    "                content = self.extract_article_content(soup)\n",
    "                if content:\n",
    "                    title = soup.find('h1').get_text(strip=True) if soup.find('h1') else \"No title\"\n",
    "                    date = (soup.find('time') or soup.find('meta', property='article:published_time') or\n",
    "                           soup.find('span', class_=lambda x: x and 'date' in x.lower()))\n",
    "                    date = date.get('datetime') if hasattr(date, 'get') else date.get_text(strip=True) if date else \"Unknown\"\n",
    "\n",
    "                    self.articles.append({\n",
    "                        'url': url,\n",
    "                        'title': title,\n",
    "                        'date': date,\n",
    "                        'content': content[:5000] + \"...\" if len(content) > 5000 else content,\n",
    "                        'depth': depth\n",
    "                    })\n",
    "                    print(f\"Article found at depth {depth}: {title[:50]}...\")\n",
    "\n",
    "            #max depth reached?\n",
    "            if depth < self.max_depth and len(self.articles) < self.max_articles:\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    absolute_url = urljoin(self.start_url, link['href'])\n",
    "                    if self.is_valid_url(absolute_url):\n",
    "                        self.queue.append((absolute_url, depth + 1))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {str(e)}\")\n",
    "\n",
    "    def crawl(self):\n",
    "        print(f\"\\nStarting crawl on {self.start_url}(max depth: {self.max_depth})\")\n",
    "        self.queue.append((self.start_url, 0))\n",
    "        while self.queue and len(self.articles) < self.max_articles:\n",
    "            url, depth = self.queue.popleft()\n",
    "            if url not in self.visited_urls:\n",
    "                self.process_page(url, depth)\n",
    "\n",
    "        print(f\"\\nCrawling complete. Found {len(self.articles)} articles.\")\n",
    "\n",
    "    def save_results(self, filename=\"news_articles.json\"):\n",
    "        import json\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.articles, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Results saved to {filename}\")\n",
    "\n",
    "    def print_summary(self):\n",
    "        print(\"\\nCollected Articles Summary:\")\n",
    "        for i, article in enumerate(self.articles, 1):\n",
    "            print(f\"\\n{i}. {article['title']}\")\n",
    "            print(f\"   Depth: {article['depth']} | Date: {article['date']}\")\n",
    "            print(f\"   URL: {article['url']}\")\n",
    "            print(f\"   Preview: {article['content'][:100]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sites={\n",
    "        'AP News': 'https://apnews.com',\n",
    "        'PBS NewsHour': 'https://www.pbs.org/newshour/',\n",
    "        'BBC News': 'https://www.bbc.com/news',\n",
    "        'NPR News': 'https://www.npr.org/sections/news/'}\n",
    "    print(\"Available crawler friendly news sites:\")\n",
    "    for i, (name, url) in enumerate(sites.items(), 1):\n",
    "        print(f\"{i}. {name} ({url})\")\n",
    "    choice = int(input(\"\\nSelect a site to crawl (1-4): \")) - 1\n",
    "    selected_url = list(sites.values())[choice]\n",
    "    crawler = News_crawler(\n",
    "        start_url=selected_url,\n",
    "        max_articles=100,\n",
    "        max_depth=5,\n",
    "        delay=1.5\n",
    "    )\n",
    "    crawler.crawl()\n",
    "    crawler.print_summary()\n",
    "    crawler.save_results()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available crawler friendly news sites:\n",
      "1. AP News (https://apnews.com)\n",
      "2. PBS NewsHour (https://www.pbs.org/newshour/)\n",
      "3. BBC News (https://www.bbc.com/news)\n",
      "4. NPR News (https://www.npr.org/sections/news/)\n",
      "\n",
      "Starting crawl on https://www.bbc.com/news(max depth: 5)\n",
      "Article found at depth 0: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: US envoy rejects Hamas claim that it has agreed to...\n",
      "Article found at depth 1: BBC Sport...\n",
      "Article found at depth 1: Business...\n",
      "Article found at depth 1: Innovation...\n",
      "Article found at depth 1: Culture...\n",
      "Article found at depth 1: Arts...\n",
      "Article found at depth 1: Travel...\n",
      "Article found at depth 1: Earth...\n",
      "Article found at depth 1: Audio...\n",
      "Article found at depth 1: Video...\n",
      "Article found at depth 1: Live Now...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: BBC Homepage...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: NewsNews...\n",
      "Article found at depth 1: Executive Lounge...\n",
      "Article found at depth 1: Technology of Business...\n",
      "Article found at depth 1: Future of Business...\n",
      "Article found at depth 1: Technology...\n",
      "Article found at depth 1: Science & Health...\n",
      "Article found at depth 1: Artificial Intelligence...\n",
      "Article found at depth 1: AI v the Mind...\n",
      "Article found at depth 1: Film & TV...\n",
      "Article found at depth 1: Music...\n",
      "Article found at depth 1: Art & Design...\n",
      "Article found at depth 1: Style...\n",
      "Article found at depth 1: Books...\n",
      "Article found at depth 1: Entertainment & Arts...\n",
      "Article found at depth 1: Arts in Motion...\n",
      "Article found at depth 1: Destinations...\n",
      "Article found at depth 1: Destinations...\n",
      "Article found at depth 1: Destinations...\n",
      "Article found at depth 1: Destinations...\n",
      "Article found at depth 1: Destinations...\n",
      "Article found at depth 1: Destinations...\n",
      "Article found at depth 1: Destinations...\n",
      "Article found at depth 1: Destinations...\n",
      "Article found at depth 1: Destinations...\n",
      "Article found at depth 1: Destinations...\n",
      "Article found at depth 1: Destinations...\n",
      "Article found at depth 1: World’s Table...\n",
      "Article found at depth 1: Culture & Experiences...\n",
      "Article found at depth 1: Adventures...\n",
      "Article found at depth 1: The SpeciaList...\n",
      "Article found at depth 1: Natural Wonders...\n",
      "Article found at depth 1: Weather & Science...\n",
      "Article found at depth 1: Climate Solutions...\n",
      "Article found at depth 1: Sustainable Business...\n",
      "Article found at depth 1: Green Living...\n",
      "Article found at depth 1: Podcasts...\n",
      "Article found at depth 1: Radio Stations...\n",
      "Error processing https://www.bbc.com/undefined: 404 Client Error: Not Found for url: https://www.bbc.com/undefined\n",
      "Article found at depth 1: Live Now...\n",
      "Article found at depth 1: Recently Live...\n",
      "Article found at depth 1: BBC Weather...\n",
      "Article found at depth 1: Dozens injured after car ploughs into Liverpool cr...\n",
      "Article found at depth 1: What we know about the Liverpool FC parade inciden...\n",
      "Article found at depth 1: Greek coastguards charged over 2023 migrant shipwr...\n",
      "Article found at depth 1: India's ex-wrestling chief cleared in minor's sexu...\n",
      "Article found at depth 1: What you need to know ahead of South Korea's snap ...\n",
      "Article found at depth 1: Moment car drives into crowd at Liverpool trophy p...\n",
      "Article found at depth 1: North Korea says US 'Golden Dome' risks 'space nuc...\n",
      "Article found at depth 1: Nepal's 'Everest Man' sets record with 31st summit...\n",
      "Article found at depth 1: From prodigy to leader: Can Shubman Gill shape the...\n",
      "Article found at depth 1: King prepares to give key speech backing Canada...\n",
      "Article found at depth 1: Chinese-owned Volvo Cars to cut 3,000 jobs...\n",
      "Article found at depth 1: BBC given rare access to search for Hezbollah posi...\n",
      "Article found at depth 1: Far-right marchers attack Palestinians as Israel m...\n",
      "Article found at depth 1: Could Nigeria's careful ethnic balancing act be un...\n",
      "Article found at depth 1: An Indian teacher was killed - then he got falsely...\n",
      "Article found at depth 1: 'My life was saved by a stranger on the other side...\n",
      "Article found at depth 1: The people who think AI might become conscious...\n",
      "Article found at depth 1: China student says college made her 'take off trou...\n",
      "Article found at depth 1: Humanoid robots fight in Chinese kick-boxing compe...\n",
      "Article found at depth 1: Watch: Push in the face by wife was joke not domes...\n",
      "Article found at depth 1: Moment truck explodes while driving through Chicag...\n",
      "Article found at depth 1: North Korea arrests senior official over warship l...\n",
      "Article found at depth 1: Fake discounts on Shein 'breach law', EU says...\n",
      "Article found at depth 1: 'Nowhere is safe' - Cameroonians trapped between s...\n",
      "\n",
      "Crawling complete. Found 100 articles.\n",
      "\n",
      "Collected Articles Summary:\n",
      "\n",
      "1. NewsNews\n",
      "   Depth: 0 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news\n",
      "   Preview: News\n",
      "News\n",
      "LIVE\n",
      "Dozens injured after car ploughs into Liverpool crowd as police say not terror-relate...\n",
      "\n",
      "2. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news#main-content\n",
      "   Preview: News\n",
      "News\n",
      "LIVE\n",
      "Dozens injured after car ploughs into Liverpool crowd as police say not terror-relate...\n",
      "\n",
      "3. US envoy rejects Hamas claim that it has agreed to American terms for a Gaza ceasefire\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/\n",
      "   Preview: What we know about the Liverpool FC parade incident\n",
      "A 53-year-old white British man has been arreste...\n",
      "\n",
      "4. BBC Sport\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/sport\n",
      "   Preview: BBC Sport\n",
      "Live\n",
      ".Â\n",
      "Dozens injured after car ploughs into Liverpool crowd as police say not terror-rel...\n",
      "\n",
      "5. Business\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/business\n",
      "   Preview: Business\n",
      "Chinese-owned Volvo Cars to cut 3,000 jobs\n",
      "The firm's boss pointed to the \"challenging peri...\n",
      "\n",
      "6. Innovation\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/innovation\n",
      "   Preview: Innovation\n",
      "Should you ever cut ties with your parents?\n",
      "Estrangement between parents and their childr...\n",
      "\n",
      "7. Culture\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/culture\n",
      "   Preview: Culture\n",
      "Rushdie 'pleased' with attacker's maximum sentence\n",
      "Author Sir Salman Rushdie says he hopes t...\n",
      "\n",
      "8. Arts\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/arts\n",
      "   Preview: Arts\n",
      "India's colonial past revealed through 200 masterful paintings\n",
      "A new show in Delhi features mor...\n",
      "\n",
      "9. Travel\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel\n",
      "   Preview: Travel\n",
      "9 family trips to take in the US this year\n",
      "Want to break free from the all-inclusive resort? ...\n",
      "\n",
      "10. Earth\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/future-planet\n",
      "   Preview: Earth\n",
      "India state on alert after ship carrying hazardous cargo capsizes\n",
      "All 24 crew members on board...\n",
      "\n",
      "11. Audio\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/audio\n",
      "   Preview: Audio\n",
      "Diddy On Trial\n",
      "Kid Cudi: ‘Porsche destroyed by Molotov cocktail’\n",
      "The rapper says it happened a...\n",
      "\n",
      "12. Video\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/video\n",
      "   Preview: Video\n",
      "Moment car drives into crowd at Liverpool trophy parade\n",
      "A 53-year-old white British man has be...\n",
      "\n",
      "13. Live Now\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/live\n",
      "   Preview: Live Now\n",
      "LIVE\n",
      "Dozens injured after car ploughs into Liverpool crowd as police say not terror-related...\n",
      "\n",
      "14. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/topics/c2vdnvdg6xxt\n",
      "   Preview: News\n",
      "News\n",
      "Israel-Gaza war\n",
      "Israeli strike kills dozens sheltering in Gaza school, officials say\n",
      "Child...\n",
      "\n",
      "15. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/war-in-ukraine\n",
      "   Preview: News\n",
      "News\n",
      "War in Ukraine\n",
      "Kremlin calls Trump 'emotional' after US president says Putin is 'crazy'\n",
      "Tr...\n",
      "\n",
      "16. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/us-canada\n",
      "   Preview: News\n",
      "News\n",
      "US & Canada\n",
      "Kremlin calls Trump 'emotional' after US president says Putin is 'crazy'\n",
      "Trump...\n",
      "\n",
      "17. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/uk\n",
      "   Preview: News\n",
      "News\n",
      "UK\n",
      "Four children injured as car hits Liverpool crowd\n",
      "Twenty-seven people were taken to hos...\n",
      "\n",
      "18. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/world/africa\n",
      "   Preview: News\n",
      "News\n",
      "Africa\n",
      "Could Nigeria's careful ethnic balancing act be under threat?\n",
      "Criticism is growing ...\n",
      "\n",
      "19. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/world/asia\n",
      "   Preview: News\n",
      "News\n",
      "Asia\n",
      "North Korea says US 'Golden Dome' risks 'space nuclear war'\n",
      "Pyongyang says Washington...\n",
      "\n",
      "20. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/world/australia\n",
      "   Preview: News\n",
      "News\n",
      "Australia\n",
      "Australia fast-tracks machete ban after shopping centre attack\n",
      "The ban follows a...\n",
      "\n",
      "21. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/world/europe\n",
      "   Preview: News\n",
      "News\n",
      "Europe\n",
      "Everyone is emotional, says Kremlin, after Trump calls Putin 'absolutely crazy'\n",
      "Tru...\n",
      "\n",
      "22. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/world/latin_america\n",
      "   Preview: News\n",
      "News\n",
      "Latin America\n",
      "Venezuela's ruling party claims election win as opposition boycotts poll\n",
      "Pre...\n",
      "\n",
      "23. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/world/middle_east\n",
      "   Preview: News\n",
      "News\n",
      "Middle East\n",
      "Israeli strike kills dozens sheltering in Gaza school, officials say\n",
      "Children ...\n",
      "\n",
      "24. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/in_pictures\n",
      "   Preview: News\n",
      "News\n",
      "In Pictures\n",
      "King Charles and Queen Camilla welcomed in Ottawa amid US tensions\n",
      "Monday was ...\n",
      "\n",
      "25. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/bbcindepth\n",
      "   Preview: News\n",
      "News\n",
      "BBC InDepth\n",
      "The people who think AI might become conscious\n",
      "With a leap in the evolution of...\n",
      "\n",
      "26. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/bbcverify\n",
      "   Preview: News\n",
      "News\n",
      "BBC Verify\n",
      "Investigating Israel's strike on Gaza's European Hospital\n",
      "BBC Verify analysed f...\n",
      "\n",
      "27. BBC Homepage\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/home\n",
      "   Preview: BBC Homepage\n",
      "Live\n",
      ".Â\n",
      "Dozens injured after car ploughs into Liverpool crowd as police say not terror-...\n",
      "\n",
      "28. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/politics\n",
      "   Preview: News\n",
      "News\n",
      "UK\n",
      "Apprenticeship shake-up to shift focus to under-21s\n",
      "The government will no longer fund ...\n",
      "\n",
      "29. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/england\n",
      "   Preview: News\n",
      "News\n",
      "UK\n",
      "LIVE\n",
      "Dozens injured after car ploughs into Liverpool crowd as police say not terror-rel...\n",
      "\n",
      "30. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/northern_ireland\n",
      "   Preview: News\n",
      "News\n",
      "N. Ireland\n",
      "NI Liverpool fan in hospital after being struck by car at parade\n",
      "Jack Trotter f...\n",
      "\n",
      "31. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/northern_ireland/northern_ireland_politics\n",
      "   Preview: News\n",
      "News\n",
      "N. Ireland\n",
      "DUP communities minister at first senior GAA game\n",
      "Before watching the match, Go...\n",
      "\n",
      "32. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/scotland\n",
      "   Preview: News\n",
      "News\n",
      "Scotland\n",
      "Dry heat to torrential rain - enter the age of 'weather whiplash'\n",
      "Our weather is ...\n",
      "\n",
      "33. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/scotland/scotland_politics\n",
      "   Preview: News\n",
      "News\n",
      "Scotland\n",
      "Swinney accuses Reform of racism over Sarwar advert\n",
      "The first minister has urged ...\n",
      "\n",
      "34. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/wales\n",
      "   Preview: News\n",
      "News\n",
      "Wales\n",
      "Thousands could unknowingly have alcohol brain damage\n",
      "Drinking 35 units of alcohol a...\n",
      "\n",
      "35. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/wales/wales_politics\n",
      "   Preview: News\n",
      "News\n",
      "Wales\n",
      "'Huge opportunity' for more family-friendly Senedd\n",
      "Campaigners call for more predict...\n",
      "\n",
      "36. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/world/asia/china\n",
      "   Preview: News\n",
      "News\n",
      "Asia\n",
      "China student says college made her 'take off trousers' for period leave\n",
      "While the co...\n",
      "\n",
      "37. NewsNews\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/world/asia/india\n",
      "   Preview: News\n",
      "News\n",
      "Asia\n",
      "From prodigy to leader: Can Shubman Gill shape the future of Indian Test cricket?\n",
      "To ...\n",
      "\n",
      "38. Executive Lounge\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/business/executive-lounge\n",
      "   Preview: Executive Lounge\n",
      "The ex-Google exec who wants to simplify your life\n",
      "Marissa Mayer, a former Google a...\n",
      "\n",
      "39. Technology of Business\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/business/technology-of-business\n",
      "   Preview: Technology of Business\n",
      "Frugal tech: The start-ups working on cheap innovation\n",
      "Indian start-ups are u...\n",
      "\n",
      "40. Future of Business\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/business/future-of-business\n",
      "   Preview: Future of Business\n",
      "Frugal tech: The start-ups working on cheap innovation\n",
      "Indian start-ups are using...\n",
      "\n",
      "41. Technology\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/innovation/technology\n",
      "   Preview: Technology\n",
      "Should children at nursery have screen time?\n",
      "The World Health Organization recommends chi...\n",
      "\n",
      "42. Science & Health\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/innovation/science\n",
      "   Preview: Science & Health\n",
      "Should you ever cut ties with your parents?\n",
      "Estrangement between parents and their ...\n",
      "\n",
      "43. Artificial Intelligence\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/innovation/artificial-intelligence\n",
      "   Preview: Artificial Intelligence\n",
      "The people who think AI might become conscious\n",
      "With a leap in the evolution ...\n",
      "\n",
      "44. AI v the Mind\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/innovation/ai-v-the-mind\n",
      "   Preview: AI v the Mind\n",
      "Meet the world's first recipient of an AI-powered bionic arm\n",
      "Sarah De Lagarde lost an ...\n",
      "\n",
      "45. Film & TV\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/culture/film-tv\n",
      "   Preview: Film & TV\n",
      "The 12 Cannes films you need to know about\n",
      "From the hundreds of titles that premiered at t...\n",
      "\n",
      "46. Music\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/culture/music\n",
      "   Preview: Music\n",
      "Billy Joel cancels tour after brain condition diagnosis\n",
      "Doctors have told the veteran US singe...\n",
      "\n",
      "47. Art & Design\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/culture/art\n",
      "   Preview: Art & Design\n",
      "India's colonial past revealed through 200 masterful paintings\n",
      "A new show in Delhi feat...\n",
      "\n",
      "48. Style\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/culture/style\n",
      "   Preview: Style\n",
      "How I learned to love my curls after years of abusing them\n",
      "I spent years abusing my juicy thic...\n",
      "\n",
      "49. Books\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/culture/books\n",
      "   Preview: Books\n",
      "Rushdie 'pleased' with attacker's maximum sentence\n",
      "Author Sir Salman Rushdie says he hopes the...\n",
      "\n",
      "50. Entertainment & Arts\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/culture/entertainment-news\n",
      "   Preview: Entertainment & Arts\n",
      "Iran's Palme d'Or-winning director cheered as he arrives home\n",
      "Director Jafar Pa...\n",
      "\n",
      "51. Arts in Motion\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/arts/arts-in-motion\n",
      "   Preview: Arts in Motion\n",
      "In a pioneering new collaboration, the BBC – in partnership with Rolex – will use its...\n",
      "\n",
      "52. Destinations\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/destinations\n",
      "   Preview: Destinations\n",
      "Africa's ice cream cafe that nurtures self-esteem\n",
      "Tapi Tapi makes ice cream with a deep...\n",
      "\n",
      "53. Destinations\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/destinations/africa\n",
      "   Preview: Destinations\n",
      "Destination dupes: Where to go instead of the US\n",
      "Big skies, bold cities and iconic pris...\n",
      "\n",
      "54. Destinations\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/destinations/antarctica\n",
      "   Preview: Destinations\n",
      "The ultimate solo adventure?\n",
      "Spanish adventure athlete Antonio de la Rosa attempted to ...\n",
      "\n",
      "55. Destinations\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/destinations/asia\n",
      "   Preview: Destinations\n",
      "The slowest train journey in India\n",
      "Nobody uses the Nilgiri Mountain Railway to get from...\n",
      "\n",
      "56. Destinations\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/destinations/australia-and-pacific\n",
      "   Preview: Destinations\n",
      "The 333 islands opening to the world\n",
      "As of 1 December, Fiji instituted a quarantine-fre...\n",
      "\n",
      "57. Destinations\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/destinations/caribbean\n",
      "   Preview: Destinations\n",
      "Natalia Vallejo's Caldo de gallina\n",
      "The James Beard award-winning chef's ancestral soup ...\n",
      "\n",
      "58. Destinations\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/destinations/central-america\n",
      "   Preview: Destinations\n",
      "Gallo pinto: Costa Rica rice and beans\n",
      "Deeply rooted in Costa Rican and Nicaraguan cult...\n",
      "\n",
      "59. Destinations\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/destinations/europe\n",
      "   Preview: Destinations\n",
      "London's newest hip area isn't where you think\n",
      "A few miles from East London's perennial...\n",
      "\n",
      "60. Destinations\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/destinations/middle-east\n",
      "   Preview: Destinations\n",
      "Iraq's answer to the pyramids\n",
      "Egypt may have the Pyramids of Giza, but Iraq has the Zig...\n",
      "\n",
      "61. Destinations\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/destinations/north-america\n",
      "   Preview: Destinations\n",
      "The US' 113-mile 'floating' highway\n",
      "Stretching 113 miles into the open ocean, this engi...\n",
      "\n",
      "62. Destinations\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/destinations/south-america\n",
      "   Preview: Destinations\n",
      "How huge statues 'walked' 900 years ago\n",
      "Living on a remote, barren isle bestowed with f...\n",
      "\n",
      "63. World’s Table\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/worlds-table\n",
      "   Preview: World’s Table\n",
      "What happens to family recipes when home is lost?\n",
      "A new book uses food to help humanis...\n",
      "\n",
      "64. Culture & Experiences\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/cultural-experiences\n",
      "   Preview: Culture & Experiences\n",
      "The 2,000-year-old language on the rise in America\n",
      "The rise of indigenous lang...\n",
      "\n",
      "65. Adventures\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/adventures\n",
      "   Preview: Adventures\n",
      "The rare 'blue ghosts' of North Carolina\n",
      "In the forests near Asheville, rare fireflies kn...\n",
      "\n",
      "66. The SpeciaList\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/travel/specialist\n",
      "   Preview: The SpeciaList\n",
      "A chef's guide to the best cacio e pepe in Rome\n",
      "The cheesy, peppery pasta dish has be...\n",
      "\n",
      "67. Natural Wonders\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/future-planet/natural-wonders\n",
      "   Preview: Natural Wonders\n",
      "Turkey's answer to 'Burning Man'\n",
      "A music and art extravaganza takes place in an 'oth...\n",
      "\n",
      "68. Weather & Science\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/future-planet/weather-science\n",
      "   Preview: Weather & Science\n",
      "Town still recovering six months on from flooding\n",
      "Chippenham was submerged after t...\n",
      "\n",
      "69. Climate Solutions\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/future-planet/solutions\n",
      "   Preview: Climate Solutions\n",
      "The Soviet nuclear plan to reverse Siberian rivers\n",
      "In the 1970s, the USSR used nuc...\n",
      "\n",
      "70. Sustainable Business\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/future-planet/sustainable-business\n",
      "   Preview: Sustainable Business\n",
      "Everyday EVs are taking a page from Formula E\n",
      "The common electric vehicle doesn...\n",
      "\n",
      "71. Green Living\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/future-planet/green-living\n",
      "   Preview: Green Living\n",
      "This machine turns carbon dioxide into fuel\n",
      "Researchers have found a way to take carbon...\n",
      "\n",
      "72. Podcasts\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/audio/categories\n",
      "   Preview: Podcasts\n",
      "Business\n",
      "Comedy\n",
      "History\n",
      "News\n",
      "Science and health\n",
      "Society and culture\n",
      "Sport\n",
      "True crime...\n",
      "\n",
      "73. Radio Stations\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/audio/stations\n",
      "   Preview: Radio Stations\n",
      "World Service\n",
      "News & views from the BBC's international radio station.\n",
      "Listen Live\n",
      "LI...\n",
      "\n",
      "74. Live Now\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/live/news\n",
      "   Preview: Live Now\n",
      "LIVE\n",
      "Dozens injured after car ploughs into Liverpool crowd as police say not terror-related...\n",
      "\n",
      "75. Recently Live\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/live/sport\n",
      "   Preview: Recently Live\n",
      "Czech Republic U17 v England U17 - England exit the U17 European Championship after la...\n",
      "\n",
      "76. BBC Weather\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/weather\n",
      "   Preview: Terms of Use\n",
      "About the BBC\n",
      "Privacy Policy\n",
      "Privacy Policy\n",
      "Cookies\n",
      "Cookies\n",
      "Accessibility Help\n",
      "Parental...\n",
      "\n",
      "77. Dozens injured after car ploughs into Liverpool crowd as police say not terror-related\n",
      "   Depth: 1 | Date: Unknown\n",
      "   URL: https://www.bbc.com/news/live/cn5xnlkegz0t\n",
      "   Preview: 'This memory will be with me all the time now'\n",
      "published at 09:58 British Summer Time\n",
      "09:58 BST\n",
      "Jaik...\n",
      "\n",
      "78. What we know about the Liverpool FC parade incident\n",
      "   Depth: 1 | Date: 2025-05-27T07:36:56.742Z\n",
      "   URL: https://www.bbc.com/news/articles/ce8209lzzp4o\n",
      "   Preview: What we know about the Liverpool FC parade incident\n",
      "1 hour ago\n",
      "Share\n",
      "Save\n",
      "Doug Faulkner\n",
      "BBC News\n",
      "Sha...\n",
      "\n",
      "79. Greek coastguards charged over 2023 migrant shipwreck\n",
      "   Depth: 1 | Date: 2025-05-26T23:17:23.133Z\n",
      "   URL: https://www.bbc.com/news/articles/cp8yjlzd3pvo\n",
      "   Preview: Greek coastguards charged over 2023 migrant shipwreck\n",
      "10 hours ago\n",
      "Share\n",
      "Save\n",
      "Nick Beake\n",
      "Europe corr...\n",
      "\n",
      "80. India's ex-wrestling chief cleared in minor's sexual harassment case\n",
      "   Depth: 1 | Date: 2025-05-27T08:44:58.835Z\n",
      "   URL: https://www.bbc.com/news/articles/cx2qd4k0nwqo\n",
      "   Preview: India's ex-wrestling chief cleared in minor's sexual harassment case\n",
      "19 minutes ago\n",
      "Share\n",
      "Save\n",
      "Meryl...\n",
      "\n",
      "81. What you need to know ahead of South Korea's snap presidential election\n",
      "   Depth: 1 | Date: 2025-05-26T22:00:14.000Z\n",
      "   URL: https://www.bbc.com/news/articles/c8e66565wj0o\n",
      "   Preview: What you need to know ahead of South Korea's snap presidential election\n",
      "11 hours ago\n",
      "Share\n",
      "Save\n",
      "Joel...\n",
      "\n",
      "82. Moment car drives into crowd at Liverpool trophy parade\n",
      "   Depth: 1 | Date: 2025-05-26T21:53:05.200Z\n",
      "   URL: https://www.bbc.com/news/videos/c4grq15y6yeo\n",
      "   Preview: Moment car drives into crowd at Liverpool trophy parade\n",
      "Four children are among the dozens of people...\n",
      "\n",
      "83. North Korea says US 'Golden Dome' risks 'space nuclear war'\n",
      "   Depth: 1 | Date: 2025-05-27T08:13:58.318Z\n",
      "   URL: https://www.bbc.com/news/articles/cd0l4e0rnr1o\n",
      "   Preview: North Korea says US 'Golden Dome' risks 'space nuclear war'\n",
      "52 minutes ago\n",
      "Share\n",
      "Save\n",
      "Kelly Ng\n",
      "BBC N...\n",
      "\n",
      "84. Nepal's 'Everest Man' sets record with 31st summit\n",
      "   Depth: 1 | Date: 2025-05-27T05:03:31.995Z\n",
      "   URL: https://www.bbc.com/news/articles/cm2y70xknnyo\n",
      "   Preview: Nepal's 'Everest Man' sets record with 31st summit\n",
      "4 hours ago\n",
      "Share\n",
      "Save\n",
      "Kelly Ng\n",
      "BBC News\n",
      "Share\n",
      "Sa...\n",
      "\n",
      "85. From prodigy to leader: Can Shubman Gill shape the future of Indian Test cricket?\n",
      "   Depth: 1 | Date: 2025-05-26T22:57:38.881Z\n",
      "   URL: https://www.bbc.com/news/articles/cx2r532gjyno\n",
      "   Preview: From prodigy to leader: Can Shubman Gill shape the future of Indian Test cricket?\n",
      "10 hours ago\n",
      "Share...\n",
      "\n",
      "86. King prepares to give key speech backing Canada\n",
      "   Depth: 1 | Date: 2025-05-27T00:25:00.026Z\n",
      "   URL: https://www.bbc.com/news/articles/c9wgd98yr89o\n",
      "   Preview: King prepares to give key speech backing Canada\n",
      "9 hours ago\n",
      "Share\n",
      "Save\n",
      "Sean Coughlan\n",
      "Royal correspon...\n",
      "\n",
      "87. Chinese-owned Volvo Cars to cut 3,000 jobs\n",
      "   Depth: 1 | Date: 2025-05-27T04:33:42.306Z\n",
      "   URL: https://www.bbc.com/news/articles/c8jgrl1wde8o\n",
      "   Preview: Chinese-owned Volvo Cars to cut 3,000 jobs\n",
      "4 hours ago\n",
      "Share\n",
      "Save\n",
      "Peter Hoskins\n",
      "Business reporter\n",
      "Sh...\n",
      "\n",
      "88. BBC given rare access to search for Hezbollah positions with UN troops in Lebanon\n",
      "   Depth: 1 | Date: 2025-05-27T05:50:11.540Z\n",
      "   URL: https://www.bbc.com/news/videos/cgj8vqng4vvo\n",
      "   Preview: BBC given rare access to search for Hezbollah positions with UN troops in Lebanon\n",
      "The BBC has been g...\n",
      "\n",
      "89. Far-right marchers attack Palestinians as Israel marks taking of Jerusalem\n",
      "   Depth: 1 | Date: 2025-05-26T18:22:43.512Z\n",
      "   URL: https://www.bbc.com/news/articles/czelwkwn3y2o\n",
      "   Preview: Far-right marchers attack Palestinians as Israel marks taking of Jerusalem\n",
      "15 hours ago\n",
      "Share\n",
      "Save\n",
      "W...\n",
      "\n",
      "90. Could Nigeria's careful ethnic balancing act be under threat?\n",
      "   Depth: 1 | Date: 2025-05-26T22:59:27.113Z\n",
      "   URL: https://www.bbc.com/news/articles/ce9v2m7k988o\n",
      "   Preview: Could Nigeria's careful ethnic balancing act be under threat?\n",
      "9 hours ago\n",
      "Share\n",
      "Save\n",
      "Mansur Abubakar...\n",
      "\n",
      "91. An Indian teacher was killed - then he got falsely labelled a \"terrorist\"\n",
      "   Depth: 1 | Date: 2025-05-25T23:07:30.979Z\n",
      "   URL: https://www.bbc.com/news/articles/c17rw0vy0glo\n",
      "   Preview: An Indian teacher was killed - then he got falsely labelled a \"terrorist\"\n",
      "1 day ago\n",
      "Share\n",
      "Save\n",
      "Chery...\n",
      "\n",
      "92. 'My life was saved by a stranger on the other side of the world'\n",
      "   Depth: 1 | Date: 2025-05-27T05:01:50.237Z\n",
      "   URL: https://www.bbc.com/news/articles/cpqe52pj1d2o\n",
      "   Preview: 'My life was saved by a stranger on the other side of the world'\n",
      "4 hours ago\n",
      "Share\n",
      "Save\n",
      "Fi Lamdin & ...\n",
      "\n",
      "93. The people who think AI might become conscious\n",
      "   Depth: 1 | Date: 2025-05-26T00:38:27.819Z\n",
      "   URL: https://www.bbc.com/news/articles/c0k3700zljjo\n",
      "   Preview: The people who think AI might become conscious\n",
      "1 day ago\n",
      "Share\n",
      "Save\n",
      "Pallab Ghosh\n",
      "Science corresponde...\n",
      "\n",
      "94. China student says college made her 'take off trousers' for period leave\n",
      "   Depth: 1 | Date: 2025-05-26T09:32:20.513Z\n",
      "   URL: https://www.bbc.com/news/articles/c8jgmwymw8eo\n",
      "   Preview: China student says college made her 'take off trousers' for period leave\n",
      "23 hours ago\n",
      "Share\n",
      "Save\n",
      "Koh...\n",
      "\n",
      "95. Humanoid robots fight in Chinese kick-boxing competition\n",
      "   Depth: 1 | Date: 2025-05-26T06:09:34.321Z\n",
      "   URL: https://www.bbc.com/news/videos/cgeg2x3lwepo\n",
      "   Preview: Humanoid robots fight in Chinese kick-boxing competition\n",
      "Two humanoid robots traded punches while fa...\n",
      "\n",
      "96. Watch: Push in the face by wife was joke not domestic dispute, says Macron\n",
      "   Depth: 1 | Date: 2025-05-26T11:41:52.240Z\n",
      "   URL: https://www.bbc.com/news/videos/c201e8g0yg3o\n",
      "   Preview: Watch: Push in the face by wife was joke not domestic dispute, says Macron\n",
      "Footage shows the moment ...\n",
      "\n",
      "97. Moment truck explodes while driving through Chicago suburb\n",
      "   Depth: 1 | Date: 2025-05-26T22:47:18.223Z\n",
      "   URL: https://www.bbc.com/news/videos/c4grjnd1dveo\n",
      "   Preview: Moment truck explodes while driving through Chicago suburb\n",
      "A man was injured and four homes were dam...\n",
      "\n",
      "98. North Korea arrests senior official over warship launch failure\n",
      "   Depth: 1 | Date: 2025-05-26T07:02:40.468Z\n",
      "   URL: https://www.bbc.com/news/articles/cwy3rr307qyo\n",
      "   Preview: North Korea arrests senior official over warship launch failure\n",
      "1 day ago\n",
      "Share\n",
      "Save\n",
      "Kelly Ng\n",
      "BBC Ne...\n",
      "\n",
      "99. Fake discounts on Shein 'breach law', EU says\n",
      "   Depth: 1 | Date: 2025-05-26T16:35:06.784Z\n",
      "   URL: https://www.bbc.com/news/articles/c62vq1mnndno\n",
      "   Preview: Fake discounts on Shein 'breach law', EU says\n",
      "17 hours ago\n",
      "Share\n",
      "Save\n",
      "Mitchell Labiak\n",
      "Business repor...\n",
      "\n",
      "100. 'Nowhere is safe' - Cameroonians trapped between separatists and soldiers\n",
      "   Depth: 1 | Date: 2025-05-25T23:08:57.867Z\n",
      "   URL: https://www.bbc.com/news/articles/c6296pp1p6wo\n",
      "   Preview: 'Nowhere is safe' - Cameroonians trapped between separatists and soldiers\n",
      "1 day ago\n",
      "Share\n",
      "Save\n",
      "Nick ...\n",
      "Results saved to news_articles.json\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T09:12:12.871532Z",
     "start_time": "2025-05-27T09:10:36.011406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Fine-tuned models (replace with your own or pre-trained alternatives)\n",
    "SECTOR_MODEL = \"yiyanghkust/finbert-tone\"  # Domain-specific BERT\n",
    "SENTIMENT_MODEL = \"ProsusAI/finbert\"  # Financially-tuned sentiment\n",
    "\n",
    "# Initialize models\n",
    "tokenizer = AutoTokenizer.from_pretrained(SECTOR_MODEL)\n",
    "sector_model = AutoModelForSequenceClassification.from_pretrained(SECTOR_MODEL)\n",
    "sentiment_analyzer = pipeline(\"text-classification\", model=SENTIMENT_MODEL)\n",
    "\n",
    "# Define stable sectors (customize as needed)\n",
    "STABLE_SECTORS = [\n",
    "    \"Technology\", \"Healthcare\", \"Energy\",\n",
    "    \"Finance\", \"Environment\", \"Industrial\"\n",
    "]\n",
    "\n",
    "# Classify sectors using BERT logits (stable predictions)\n",
    "def classify_sector(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = sector_model(**inputs)\n",
    "    logits = outputs.logits.detach().numpy()\n",
    "    return STABLE_SECTORS[np.argmax(logits)]\n",
    "\n",
    "# Analyze sentiment with FinBERT (economic focus)\n",
    "def analyze_sentiment(text):\n",
    "    result = sentiment_analyzer(text)[0]\n",
    "    return 1 if result[\"label\"] == \"positive\" else -1\n",
    "\n",
    "# Process news with rolling averages (reduces noise)\n",
    "def analyze_news_stable(news_items, window_size=5):\n",
    "    sector_scores = defaultdict(list)\n",
    "\n",
    "    for i, item in enumerate(news_items):\n",
    "        sector = classify_sector(item[\"content\"])\n",
    "        sentiment = analyze_sentiment(item[\"content\"])\n",
    "\n",
    "        # Apply rolling window smoothing\n",
    "        if len(sector_scores[sector]) >= window_size:\n",
    "            sector_scores[sector].pop(0)\n",
    "        sector_scores[sector].append(sentiment)\n",
    "\n",
    "    # Calculate growth (smoothed)\n",
    "    growth_estimates = {}\n",
    "    for sector, scores in sector_scores.items():\n",
    "        smoothed_score = mean(scores) * 2  # Scale to %\n",
    "        growth_estimates[sector] = f\"{smoothed_score:.1f}%\"\n",
    "\n",
    "    return growth_estimates\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    news_data = load_data(\"news_articles.json\")  # Your 100-item dataset\n",
    "    growth_report = analyze_news_stable(news_data)\n",
    "\n",
    "    print(\"Stable Sector Growth Projections:\")\n",
    "    for sector, growth in growth_report.items():\n",
    "        print(f\"{sector}: {growth}\")"
   ],
   "id": "c775333500d872c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/533 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2042ff380971420fbe5b2b571e070315"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krinal\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Krinal\\.cache\\huggingface\\hub\\models--yiyanghkust--finbert-tone. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "625d56b8fbd44d83be064ae777e5c5a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88cd0082e361466fa6902b400a6632cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b216b1178df2478f9f162e0b90343a17"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krinal\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Krinal\\.cache\\huggingface\\hub\\models--ProsusAI--finbert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e202fbd6131643d39b8c105b3b55b8fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0112a000b17948b991f3334a33937ed9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00f5565631904b7a86dca91b74a1c540"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "167af12eb3e94ef7b5ebe11357f4493a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d33dc20b60240a08d5a233df27336a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44bd38c43c7349ba865808272604e06a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1058 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1058) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 66\u001B[39m\n\u001B[32m     64\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     65\u001B[39m     news_data = load_data(\u001B[33m\"\u001B[39m\u001B[33mnews_articles.json\u001B[39m\u001B[33m\"\u001B[39m)  \u001B[38;5;66;03m# Your 100-item dataset\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m66\u001B[39m     growth_report = \u001B[43manalyze_news_stable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnews_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     68\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mStable Sector Growth Projections:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     69\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m sector, growth \u001B[38;5;129;01min\u001B[39;00m growth_report.items():\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 48\u001B[39m, in \u001B[36manalyze_news_stable\u001B[39m\u001B[34m(news_items, window_size)\u001B[39m\n\u001B[32m     46\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, item \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(news_items):\n\u001B[32m     47\u001B[39m     sector = classify_sector(item[\u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m---> \u001B[39m\u001B[32m48\u001B[39m     sentiment = \u001B[43manalyze_sentiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcontent\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     50\u001B[39m     \u001B[38;5;66;03m# Apply rolling window smoothing\u001B[39;00m\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(sector_scores[sector]) >= window_size:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 39\u001B[39m, in \u001B[36manalyze_sentiment\u001B[39m\u001B[34m(text)\u001B[39m\n\u001B[32m     38\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34manalyze_sentiment\u001B[39m(text):\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m     result = \u001B[43msentiment_analyzer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]\n\u001B[32m     40\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[32m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result[\u001B[33m\"\u001B[39m\u001B[33mlabel\u001B[39m\u001B[33m\"\u001B[39m] == \u001B[33m\"\u001B[39m\u001B[33mpositive\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m -\u001B[32m1\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:159\u001B[39m, in \u001B[36mTextClassificationPipeline.__call__\u001B[39m\u001B[34m(self, inputs, **kwargs)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    125\u001B[39m \u001B[33;03mClassify the text(s) given as inputs.\u001B[39;00m\n\u001B[32m    126\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    156\u001B[39m \u001B[33;03m    If `top_k` is used, one such dictionary is returned per label.\u001B[39;00m\n\u001B[32m    157\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    158\u001B[39m inputs = (inputs,)\n\u001B[32m--> \u001B[39m\u001B[32m159\u001B[39m result = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001B[39;00m\n\u001B[32m    161\u001B[39m _legacy = \u001B[33m\"\u001B[39m\u001B[33mtop_k\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m kwargs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1431\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1423\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[32m   1424\u001B[39m         \u001B[38;5;28miter\u001B[39m(\n\u001B[32m   1425\u001B[39m             \u001B[38;5;28mself\u001B[39m.get_iterator(\n\u001B[32m   (...)\u001B[39m\u001B[32m   1428\u001B[39m         )\n\u001B[32m   1429\u001B[39m     )\n\u001B[32m   1430\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1431\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1438\u001B[39m, in \u001B[36mPipeline.run_single\u001B[39m\u001B[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[39m\n\u001B[32m   1436\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[32m   1437\u001B[39m     model_inputs = \u001B[38;5;28mself\u001B[39m.preprocess(inputs, **preprocess_params)\n\u001B[32m-> \u001B[39m\u001B[32m1438\u001B[39m     model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1439\u001B[39m     outputs = \u001B[38;5;28mself\u001B[39m.postprocess(model_outputs, **postprocess_params)\n\u001B[32m   1440\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1338\u001B[39m, in \u001B[36mPipeline.forward\u001B[39m\u001B[34m(self, model_inputs, **forward_params)\u001B[39m\n\u001B[32m   1336\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[32m   1337\u001B[39m         model_inputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_inputs, device=\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m-> \u001B[39m\u001B[32m1338\u001B[39m         model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1339\u001B[39m         model_outputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m   1340\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:190\u001B[39m, in \u001B[36mTextClassificationPipeline._forward\u001B[39m\u001B[34m(self, model_inputs)\u001B[39m\n\u001B[32m    188\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33muse_cache\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m inspect.signature(model_forward).parameters.keys():\n\u001B[32m    189\u001B[39m     model_inputs[\u001B[33m\"\u001B[39m\u001B[33muse_cache\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m190\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1503\u001B[39m, in \u001B[36mBertForSequenceClassification.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1495\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1496\u001B[39m \u001B[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[32m   1497\u001B[39m \u001B[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[32m   1498\u001B[39m \u001B[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[32m   1499\u001B[39m \u001B[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[32m   1500\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1501\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m-> \u001B[39m\u001B[32m1503\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1504\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1505\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1506\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1507\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1508\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1509\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1510\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1511\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1512\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1513\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1515\u001B[39m pooled_output = outputs[\u001B[32m1\u001B[39m]\n\u001B[32m   1517\u001B[39m pooled_output = \u001B[38;5;28mself\u001B[39m.dropout(pooled_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:952\u001B[39m, in \u001B[36mBertModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m    949\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    950\u001B[39m         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001B[32m--> \u001B[39m\u001B[32m952\u001B[39m embedding_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    953\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    954\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    955\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    956\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    957\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    958\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    960\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    961\u001B[39m     attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:184\u001B[39m, in \u001B[36mBertEmbeddings.forward\u001B[39m\u001B[34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001B[39m\n\u001B[32m    182\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.position_embedding_type == \u001B[33m\"\u001B[39m\u001B[33mabsolute\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    183\u001B[39m     position_embeddings = \u001B[38;5;28mself\u001B[39m.position_embeddings(position_ids)\n\u001B[32m--> \u001B[39m\u001B[32m184\u001B[39m     \u001B[43membeddings\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_embeddings\u001B[49m\n\u001B[32m    185\u001B[39m embeddings = \u001B[38;5;28mself\u001B[39m.LayerNorm(embeddings)\n\u001B[32m    186\u001B[39m embeddings = \u001B[38;5;28mself\u001B[39m.dropout(embeddings)\n",
      "\u001B[31mRuntimeError\u001B[39m: The size of tensor a (1058) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T09:25:47.627531Z",
     "start_time": "2025-05-27T09:25:28.446189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "\n",
    "# Load data with proper error handling\n",
    "def load_data(file_path=\"news_articles.json\"):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Initialize models with error handling\n",
    "try:\n",
    "    SECTOR_MODEL = \"yiyanghkust/finbert-tone\"\n",
    "    SENTIMENT_MODEL = \"ProsusAI/finbert\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(SECTOR_MODEL)\n",
    "    sector_model = AutoModelForSequenceClassification.from_pretrained(SECTOR_MODEL)\n",
    "    sentiment_analyzer = pipeline(\"text-classification\", model=SENTIMENT_MODEL,\n",
    "                               tokenizer=SENTIMENT_MODEL, device=-1)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "STABLE_SECTORS = [\"Technology\", \"Healthcare\", \"Energy\", \"Finance\", \"Environment\", \"Industrial\"]\n",
    "\n",
    "def classify_sector(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text[:512], return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        outputs = sector_model(**inputs)\n",
    "        return STABLE_SECTORS[np.argmax(outputs.logits.detach().numpy())]\n",
    "    except Exception as e:\n",
    "        print(f\"Sector classification error: {str(e)}\")\n",
    "        return \"Unknown\"\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        truncated_text = \" \".join(text.split()[:500])\n",
    "        result = sentiment_analyzer(truncated_text)[0]\n",
    "        return 1 if result[\"label\"] == \"positive\" else -1\n",
    "    except Exception as e:\n",
    "        print(f\"Sentiment analysis error: {str(e)}\")\n",
    "        return 0  # Neutral if error occurs\n",
    "\n",
    "def analyze_news_stable(news_items, window_size=5):\n",
    "    sector_scores = defaultdict(list)\n",
    "\n",
    "    for item in news_items:\n",
    "        if not isinstance(item, dict) or \"content\" not in item:\n",
    "            continue\n",
    "\n",
    "        sector = classify_sector(item[\"content\"])\n",
    "        sentiment = analyze_sentiment(item[\"content\"])\n",
    "\n",
    "        if len(sector_scores[sector]) >= window_size:\n",
    "            sector_scores[sector].pop(0)\n",
    "        sector_scores[sector].append(sentiment)\n",
    "\n",
    "    growth_estimates = {}\n",
    "    for sector, scores in sector_scores.items():\n",
    "        if scores:  # Only calculate if we have data\n",
    "            smoothed_score = mean(scores) * 2  # Scale to %\n",
    "            growth_estimates[sector] = f\"{smoothed_score:.1f}%\"\n",
    "\n",
    "    return growth_estimates\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    news_data = load_data()\n",
    "    if news_data:\n",
    "        growth_report = analyze_news_stable(news_data)\n",
    "        print(\"Stable Sector Growth Projections:\")\n",
    "        for sector, growth in growth_report.items():\n",
    "            print(f\"{sector}: {growth}\")"
   ],
   "id": "936e702322f47016",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis error: The size of tensor a (637) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (637) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (623) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (746) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (623) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (594) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (657) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (618) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (650) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (615) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (623) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (621) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (608) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (629) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (615) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (592) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (623) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (628) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (624) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (611) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (618) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (635) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (616) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (633) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (630) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (587) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (684) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (627) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (622) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (616) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (541) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (633) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (630) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (605) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (614) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (613) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (683) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (613) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (628) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (614) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (609) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (608) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (614) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (638) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (617) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (658) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (647) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (611) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (640) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (635) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (688) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (651) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (601) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (633) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (660) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (688) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (627) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (645) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (626) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (652) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (664) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (633) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (646) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (662) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (664) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (643) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (611) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (596) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (610) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (638) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (609) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (618) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (615) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (617) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (598) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (646) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (553) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (691) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (624) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (598) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (589) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (648) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (661) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (645) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (649) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (630) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (636) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (661) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (635) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (640) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (595) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (584) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Sentiment analysis error: The size of tensor a (652) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Stable Sector Growth Projections:\n",
      "Technology: 0.0%\n",
      "Healthcare: 0.0%\n",
      "Energy: 0.0%\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T10:28:42.203225Z",
     "start_time": "2025-05-27T10:28:35.454820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "import math\n",
    "from typing import Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "@dataclass\n",
    "class SectorImpact:\n",
    "    sector: str\n",
    "    impact_percentage: float\n",
    "    confidence_score: float\n",
    "    key_factors: List[str]\n",
    "    impact_type: str  # positive, negative, neutral\n",
    "\n",
    "class NewsSectorAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Comprehensive sector definitions with keywords and impact indicators\n",
    "        self.sector_definitions = {\n",
    "            'E-commerce/Online Retail': {\n",
    "                'primary_keywords': ['e-commerce', 'online shopping', 'retail platform', 'marketplace', 'digital retail'],\n",
    "                'secondary_keywords': ['consumer', 'shopping', 'discount', 'sales', 'online store', 'website'],\n",
    "                'impact_indicators': {\n",
    "                    'regulatory': ['regulation', 'law', 'compliance', 'fine', 'penalty', 'breach'],\n",
    "                    'market': ['competition', 'market share', 'growth', 'decline', 'expansion'],\n",
    "                    'operational': ['supply chain', 'logistics', 'delivery', 'customer service']\n",
    "                }\n",
    "            },\n",
    "            'Fashion/Apparel': {\n",
    "                'primary_keywords': ['fashion', 'clothing', 'apparel', 'textile', 'garment', 'fast fashion'],\n",
    "                'secondary_keywords': ['design', 'manufacturing', 'brand', 'style', 'trend'],\n",
    "                'impact_indicators': {\n",
    "                    'manufacturing': ['factory', 'production', 'labor', 'working conditions', 'supply chain'],\n",
    "                    'sustainability': ['environmental', 'sustainable', 'waste', 'carbon footprint'],\n",
    "                    'market': ['sales', 'demand', 'consumer preference', 'pricing']\n",
    "                }\n",
    "            },\n",
    "            'Technology/Digital Platforms': {\n",
    "                'primary_keywords': ['technology', 'digital platform', 'software', 'app', 'website', 'AI'],\n",
    "                'secondary_keywords': ['innovation', 'digital transformation', 'automation', 'data'],\n",
    "                'impact_indicators': {\n",
    "                    'regulatory': ['data protection', 'privacy', 'algorithm', 'content moderation'],\n",
    "                    'market': ['user adoption', 'platform growth', 'digital services'],\n",
    "                    'security': ['cybersecurity', 'data breach', 'privacy violation']\n",
    "                }\n",
    "            },\n",
    "            'Financial Services': {\n",
    "                'primary_keywords': ['banking', 'finance', 'payment', 'fintech', 'investment', 'lending'],\n",
    "                'secondary_keywords': ['transaction', 'credit', 'insurance', 'wealth management'],\n",
    "                'impact_indicators': {\n",
    "                    'regulatory': ['financial regulation', 'compliance', 'central bank', 'monetary policy'],\n",
    "                    'market': ['interest rates', 'credit risk', 'market volatility'],\n",
    "                    'technology': ['digital banking', 'blockchain', 'cryptocurrency']\n",
    "                }\n",
    "            },\n",
    "            'Healthcare': {\n",
    "                'primary_keywords': ['healthcare', 'medical', 'pharmaceutical', 'hospital', 'health'],\n",
    "                'secondary_keywords': ['patient', 'treatment', 'drug', 'medicine', 'clinical'],\n",
    "                'impact_indicators': {\n",
    "                    'regulatory': ['FDA approval', 'medical regulation', 'health policy'],\n",
    "                    'innovation': ['medical technology', 'research', 'clinical trials'],\n",
    "                    'access': ['healthcare access', 'cost', 'insurance coverage']\n",
    "                }\n",
    "            },\n",
    "            'Manufacturing': {\n",
    "                'primary_keywords': ['manufacturing', 'production', 'factory', 'industrial', 'assembly'],\n",
    "                'secondary_keywords': ['automation', 'quality control', 'supply chain', 'raw materials'],\n",
    "                'impact_indicators': {\n",
    "                    'operational': ['production capacity', 'efficiency', 'cost reduction'],\n",
    "                    'supply_chain': ['supplier', 'logistics', 'inventory', 'raw materials'],\n",
    "                    'labor': ['workforce', 'employment', 'skills', 'automation']\n",
    "                }\n",
    "            },\n",
    "            'Transportation/Logistics': {\n",
    "                'primary_keywords': ['transportation', 'logistics', 'shipping', 'delivery', 'freight'],\n",
    "                'secondary_keywords': ['supply chain', 'distribution', 'warehouse', 'fleet'],\n",
    "                'impact_indicators': {\n",
    "                    'infrastructure': ['roads', 'ports', 'airports', 'railways'],\n",
    "                    'regulation': ['transportation policy', 'safety regulations', 'emissions'],\n",
    "                    'technology': ['autonomous vehicles', 'route optimization', 'tracking']\n",
    "                }\n",
    "            },\n",
    "            'Energy': {\n",
    "                'primary_keywords': ['energy', 'power', 'electricity', 'renewable', 'oil', 'gas'],\n",
    "                'secondary_keywords': ['solar', 'wind', 'nuclear', 'fossil fuel', 'grid'],\n",
    "                'impact_indicators': {\n",
    "                    'policy': ['energy policy', 'carbon tax', 'emissions regulations'],\n",
    "                    'market': ['energy prices', 'demand', 'supply', 'capacity'],\n",
    "                    'technology': ['clean energy', 'energy storage', 'smart grid']\n",
    "                }\n",
    "            },\n",
    "            'Agriculture/Food': {\n",
    "                'primary_keywords': ['agriculture', 'farming', 'food', 'crop', 'livestock'],\n",
    "                'secondary_keywords': ['harvest', 'produce', 'agricultural', 'rural', 'farmer'],\n",
    "                'impact_indicators': {\n",
    "                    'climate': ['weather', 'drought', 'climate change', 'seasonal'],\n",
    "                    'policy': ['agricultural policy', 'subsidies', 'trade agreements'],\n",
    "                    'technology': ['precision farming', 'biotechnology', 'sustainable farming']\n",
    "                }\n",
    "            },\n",
    "            'Real Estate': {\n",
    "                'primary_keywords': ['real estate', 'property', 'housing', 'construction', 'development'],\n",
    "                'secondary_keywords': ['residential', 'commercial', 'mortgage', 'rent', 'building'],\n",
    "                'impact_indicators': {\n",
    "                    'market': ['property prices', 'housing demand', 'construction activity'],\n",
    "                    'policy': ['zoning laws', 'building codes', 'tax policy'],\n",
    "                    'finance': ['mortgage rates', 'property investment', 'real estate financing']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Impact calculation weights\n",
    "        self.impact_weights = {\n",
    "            'direct_mention': 0.4,\n",
    "            'keyword_relevance': 0.3,\n",
    "            'context_impact': 0.2,\n",
    "            'sentiment_impact': 0.1\n",
    "        }\n",
    "\n",
    "        # Initialize stop words for text processing\n",
    "        self.stop_words = {\n",
    "            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',\n",
    "            'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "            'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must',\n",
    "            'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'\n",
    "        }\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and preprocess text for analysis\"\"\"\n",
    "        # Remove extra whitespaces and normalize\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        # Convert to lowercase for consistent analysis\n",
    "        return text.lower()\n",
    "\n",
    "    def calculate_keyword_relevance(self, text: str, sector: str) -> float:\n",
    "        \"\"\"Calculate relevance score based on keyword matching\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        sector_def = self.sector_definitions[sector]\n",
    "\n",
    "        # Primary keywords have higher weight\n",
    "        primary_score = sum(2 for keyword in sector_def['primary_keywords']\n",
    "                          if keyword in text_lower)\n",
    "\n",
    "        # Secondary keywords have lower weight\n",
    "        secondary_score = sum(1 for keyword in sector_def['secondary_keywords']\n",
    "                            if keyword in text_lower)\n",
    "\n",
    "        # Impact indicators add context-specific relevance\n",
    "        impact_score = 0\n",
    "        for category, indicators in sector_def['impact_indicators'].items():\n",
    "            impact_score += sum(1.5 for indicator in indicators if indicator in text_lower)\n",
    "\n",
    "        total_score = primary_score + secondary_score + impact_score\n",
    "\n",
    "        # Normalize score (max possible score estimation)\n",
    "        max_possible = len(sector_def['primary_keywords']) * 2 + \\\n",
    "                      len(sector_def['secondary_keywords']) + \\\n",
    "                      sum(len(indicators) * 1.5 for indicators in sector_def['impact_indicators'].values())\n",
    "\n",
    "        return min(total_score / max_possible, 1.0) if max_possible > 0 else 0\n",
    "\n",
    "    def calculate_tf_idf_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate TF-IDF similarity between two texts without sklearn\"\"\"\n",
    "\n",
    "        def get_word_frequencies(text):\n",
    "            words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "            words = [word for word in words if word not in self.stop_words and len(word) > 2]\n",
    "            return Counter(words)\n",
    "\n",
    "        def calculate_tf(word_freq, total_words):\n",
    "            return {word: freq/total_words for word, freq in word_freq.items()}\n",
    "\n",
    "        def calculate_idf(word, all_docs):\n",
    "            containing_docs = sum(1 for doc in all_docs if word in doc)\n",
    "            if containing_docs == 0:\n",
    "                return 0\n",
    "            return math.log(len(all_docs) / containing_docs)\n",
    "\n",
    "        # Get word frequencies\n",
    "        freq1 = get_word_frequencies(text1)\n",
    "        freq2 = get_word_frequencies(text2)\n",
    "\n",
    "        if not freq1 or not freq2:\n",
    "            return 0.0\n",
    "\n",
    "        # Calculate TF\n",
    "        tf1 = calculate_tf(freq1, sum(freq1.values()))\n",
    "        tf2 = calculate_tf(freq2, sum(freq2.values()))\n",
    "\n",
    "        # Get all unique words\n",
    "        all_words = set(tf1.keys()) | set(tf2.keys())\n",
    "        docs = [freq1, freq2]\n",
    "\n",
    "        # Calculate TF-IDF vectors\n",
    "        tfidf1 = {}\n",
    "        tfidf2 = {}\n",
    "\n",
    "        for word in all_words:\n",
    "            idf = calculate_idf(word, docs)\n",
    "            tfidf1[word] = tf1.get(word, 0) * idf\n",
    "            tfidf2[word] = tf2.get(word, 0) * idf\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        dot_product = sum(tfidf1[word] * tfidf2[word] for word in all_words)\n",
    "\n",
    "        norm1 = math.sqrt(sum(val**2 for val in tfidf1.values()))\n",
    "        norm2 = math.sqrt(sum(val**2 for val in tfidf2.values()))\n",
    "\n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return dot_product / (norm1 * norm2)\n",
    "\n",
    "    def calculate_semantic_similarity(self, text: str, sector: str) -> float:\n",
    "        \"\"\"Calculate semantic similarity using custom TF-IDF implementation\"\"\"\n",
    "        sector_def = self.sector_definitions[sector]\n",
    "\n",
    "        # Create sector reference text\n",
    "        sector_text = ' '.join(\n",
    "            sector_def['primary_keywords'] +\n",
    "            sector_def['secondary_keywords'] +\n",
    "            [indicator for indicators in sector_def['impact_indicators'].values()\n",
    "             for indicator in indicators]\n",
    "        )\n",
    "\n",
    "        return self.calculate_tf_idf_similarity(text, sector_text)\n",
    "\n",
    "    def analyze_impact_sentiment(self, text: str, sector: str) -> Tuple[str, float]:\n",
    "        \"\"\"Analyze whether the impact is positive, negative, or neutral\"\"\"\n",
    "        positive_indicators = [\n",
    "            'growth', 'increase', 'expansion', 'success', 'improvement', 'benefit',\n",
    "            'opportunity', 'boost', 'rise', 'gain', 'advance', 'progress'\n",
    "        ]\n",
    "\n",
    "        negative_indicators = [\n",
    "            'decline', 'decrease', 'loss', 'breach', 'violation', 'fine', 'penalty',\n",
    "            'crisis', 'problem', 'issue', 'concern', 'risk', 'threat', 'challenge',\n",
    "            'drop', 'fall', 'cut', 'reduce', 'harm', 'damage'\n",
    "        ]\n",
    "\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        positive_count = sum(1 for indicator in positive_indicators if indicator in text_lower)\n",
    "        negative_count = sum(1 for indicator in negative_indicators if indicator in text_lower)\n",
    "\n",
    "        if negative_count > positive_count:\n",
    "            return 'negative', min(negative_count / (negative_count + positive_count + 1), 1.0)\n",
    "        elif positive_count > negative_count:\n",
    "            return 'positive', min(positive_count / (negative_count + positive_count + 1), 1.0)\n",
    "        else:\n",
    "            return 'neutral', 0.5\n",
    "\n",
    "    def extract_key_factors(self, text: str, sector: str) -> List[str]:\n",
    "        \"\"\"Extract key factors that contribute to sector impact\"\"\"\n",
    "        factors = []\n",
    "        text_lower = text.lower()\n",
    "        sector_def = self.sector_definitions[sector]\n",
    "\n",
    "        # Check impact indicators\n",
    "        for category, indicators in sector_def['impact_indicators'].items():\n",
    "            found_indicators = [indicator for indicator in indicators if indicator in text_lower]\n",
    "            if found_indicators:\n",
    "                factors.extend(found_indicators)\n",
    "\n",
    "        # Extract sentences containing sector-relevant information\n",
    "        sentences = text.split('.')\n",
    "        relevant_sentences = []\n",
    "\n",
    "        for sentence in sentences[:5]:  # Limit to first 5 sentences for key factors\n",
    "            sentence_lower = sentence.lower()\n",
    "            if any(keyword in sentence_lower for keyword in\n",
    "                   sector_def['primary_keywords'] + sector_def['secondary_keywords']):\n",
    "                relevant_sentences.append(sentence.strip())\n",
    "\n",
    "        return factors + relevant_sentences\n",
    "\n",
    "    def calculate_confidence_score(self, keyword_relevance: float, semantic_similarity: float,\n",
    "                                 direct_mentions: int) -> float:\n",
    "        \"\"\"Calculate confidence score for the sector impact assessment\"\"\"\n",
    "        # Base confidence from relevance and similarity\n",
    "        base_confidence = (keyword_relevance * 0.6 + semantic_similarity * 0.4)\n",
    "\n",
    "        # Boost confidence with direct mentions\n",
    "        mention_boost = min(direct_mentions * 0.1, 0.3)\n",
    "\n",
    "        # Final confidence score\n",
    "        confidence = min(base_confidence + mention_boost, 1.0)\n",
    "\n",
    "        return confidence\n",
    "\n",
    "    def analyze_single_article(self, article_data: Dict) -> List[SectorImpact]:\n",
    "        \"\"\"Analyze a single article and return sector impacts\"\"\"\n",
    "        title = article_data.get('title', '')\n",
    "        content = article_data.get('content', '')\n",
    "\n",
    "        # Combine title and content for analysis (title has more weight)\n",
    "        full_text = f\"{title} {title} {content}\"  # Title repeated for emphasis\n",
    "        processed_text = self.preprocess_text(full_text)\n",
    "\n",
    "        sector_impacts = []\n",
    "\n",
    "        for sector in self.sector_definitions.keys():\n",
    "            # Calculate various relevance scores\n",
    "            keyword_relevance = self.calculate_keyword_relevance(processed_text, sector)\n",
    "            semantic_similarity = self.calculate_semantic_similarity(processed_text, sector)\n",
    "\n",
    "            # Count direct mentions of sector-related terms\n",
    "            sector_terms = (self.sector_definitions[sector]['primary_keywords'] +\n",
    "                          self.sector_definitions[sector]['secondary_keywords'])\n",
    "            direct_mentions = sum(1 for term in sector_terms if term in processed_text)\n",
    "\n",
    "            # Calculate overall impact percentage\n",
    "            impact_percentage = (\n",
    "                keyword_relevance * self.impact_weights['keyword_relevance'] +\n",
    "                semantic_similarity * self.impact_weights['context_impact'] +\n",
    "                min(direct_mentions * 0.1, 0.4) * self.impact_weights['direct_mention']\n",
    "            ) * 100\n",
    "\n",
    "            # Only include sectors with meaningful impact (threshold: 10%)\n",
    "            if impact_percentage >= 10:\n",
    "                # Analyze sentiment and extract factors\n",
    "                impact_type, sentiment_strength = self.analyze_impact_sentiment(processed_text, sector)\n",
    "                key_factors = self.extract_key_factors(content, sector)\n",
    "                confidence = self.calculate_confidence_score(keyword_relevance, semantic_similarity, direct_mentions)\n",
    "\n",
    "                # Adjust impact percentage based on sentiment strength\n",
    "                adjusted_impact = impact_percentage * (0.7 + sentiment_strength * 0.3)\n",
    "\n",
    "                sector_impacts.append(SectorImpact(\n",
    "                    sector=sector,\n",
    "                    impact_percentage=round(adjusted_impact, 2),\n",
    "                    confidence_score=round(confidence, 3),\n",
    "                    key_factors=key_factors[:5],  # Top 5 factors\n",
    "                    impact_type=impact_type\n",
    "                ))\n",
    "\n",
    "        # Sort by impact percentage and return top impacts\n",
    "        sector_impacts.sort(key=lambda x: x.impact_percentage, reverse=True)\n",
    "        return sector_impacts[:5]  # Return top 5 impacted sectors\n",
    "\n",
    "    def analyze_articles_batch(self, articles: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze multiple articles and provide comprehensive sector impact analysis\"\"\"\n",
    "        all_impacts = []\n",
    "        sector_aggregate = defaultdict(list)\n",
    "\n",
    "        for i, article in enumerate(articles):\n",
    "            print(f\"Analyzing article {i+1}/{len(articles)}: {article.get('title', 'Untitled')[:50]}...\")\n",
    "\n",
    "            impacts = self.analyze_single_article(article)\n",
    "            all_impacts.append({\n",
    "                'article_title': article.get('title', 'Untitled'),\n",
    "                'article_url': article.get('url', ''),\n",
    "                'impacts': impacts\n",
    "            })\n",
    "\n",
    "            # Aggregate sector impacts\n",
    "            for impact in impacts:\n",
    "                sector_aggregate[impact.sector].append(impact.impact_percentage)\n",
    "\n",
    "        # Calculate sector-wise statistics\n",
    "        sector_summary = {}\n",
    "        for sector, percentages in sector_aggregate.items():\n",
    "            sector_summary[sector] = {\n",
    "                'average_impact': round(sum(percentages) / len(percentages), 2),\n",
    "                'max_impact': round(max(percentages), 2),\n",
    "                'frequency': len(percentages),\n",
    "                'total_articles': len(articles),\n",
    "                'coverage_percentage': round((len(percentages) / len(articles)) * 100, 2)\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'individual_analyses': all_impacts,\n",
    "            'sector_summary': sector_summary,\n",
    "            'total_articles_analyzed': len(articles)\n",
    "        }\n",
    "\n",
    "    def generate_report(self, analysis_results: Dict) -> str:\n",
    "        \"\"\"Generate a comprehensive analysis report\"\"\"\n",
    "        report = []\n",
    "        report.append(\"=\" * 80)\n",
    "        report.append(\"NEWS ARTICLE SECTOR IMPACT ANALYSIS REPORT\")\n",
    "        report.append(\"=\" * 80)\n",
    "        report.append(f\"\\nTotal Articles Analyzed: {analysis_results['total_articles_analyzed']}\\n\")\n",
    "\n",
    "        # Sector Summary\n",
    "        report.append(\"SECTOR IMPACT SUMMARY\")\n",
    "        report.append(\"-\" * 40)\n",
    "\n",
    "        # Sort sectors by average impact\n",
    "        sorted_sectors = sorted(analysis_results['sector_summary'].items(),\n",
    "                              key=lambda x: x[1]['average_impact'], reverse=True)\n",
    "\n",
    "        for sector, stats in sorted_sectors:\n",
    "            report.append(f\"\\n{sector}:\")\n",
    "            report.append(f\"  Average Impact: {stats['average_impact']}%\")\n",
    "            report.append(f\"  Maximum Impact: {stats['max_impact']}%\")\n",
    "            report.append(f\"  Frequency: {stats['frequency']} articles\")\n",
    "            report.append(f\"  Coverage: {stats['coverage_percentage']}% of all articles\")\n",
    "\n",
    "        # Top Individual Impacts\n",
    "        report.append(f\"\\n\\nTOP INDIVIDUAL ARTICLE IMPACTS\")\n",
    "        report.append(\"-\" * 40)\n",
    "\n",
    "        top_impacts = []\n",
    "        for article_analysis in analysis_results['individual_analyses']:\n",
    "            for impact in article_analysis['impacts']:\n",
    "                top_impacts.append({\n",
    "                    'title': article_analysis['article_title'],\n",
    "                    'sector': impact.sector,\n",
    "                    'impact': impact.impact_percentage,\n",
    "                    'confidence': impact.confidence_score,\n",
    "                    'type': impact.impact_type\n",
    "                })\n",
    "\n",
    "        top_impacts.sort(key=lambda x: x['impact'], reverse=True)\n",
    "\n",
    "        for i, impact in enumerate(top_impacts[:10]):  # Top 10\n",
    "            report.append(f\"\\n{i+1}. {impact['title'][:60]}...\")\n",
    "            report.append(f\"   Sector: {impact['sector']}\")\n",
    "            report.append(f\"   Impact: {impact['impact']}% ({impact['type']})\")\n",
    "            report.append(f\"   Confidence: {impact['confidence']}\")\n",
    "\n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "    def load_articles_from_json(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Load articles from JSON file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            # Handle different JSON structures\n",
    "            if isinstance(data, list):\n",
    "                # Direct list of articles\n",
    "                articles = data\n",
    "            elif isinstance(data, dict):\n",
    "                # Check common keys for article lists\n",
    "                if 'articles' in data:\n",
    "                    articles = data['articles']\n",
    "                elif 'data' in data:\n",
    "                    articles = data['data']\n",
    "                elif 'items' in data:\n",
    "                    articles = data['items']\n",
    "                else:\n",
    "                    # Assume the dict values contain articles\n",
    "                    articles = list(data.values())\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported JSON structure\")\n",
    "\n",
    "            print(f\"Successfully loaded {len(articles)} articles from {file_path}\")\n",
    "            return articles\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File {file_path} not found\")\n",
    "            return []\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error: Invalid JSON format - {e}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading articles: {e}\")\n",
    "            return []\n",
    "\n",
    "    def save_results_to_json(self, results: Dict, output_file: str):\n",
    "        \"\"\"Save analysis results to JSON file\"\"\"\n",
    "        try:\n",
    "            # Convert dataclass objects to dictionaries for JSON serialization\n",
    "            serializable_results = {\n",
    "                'total_articles_analyzed': results['total_articles_analyzed'],\n",
    "                'sector_summary': results['sector_summary'],\n",
    "                'individual_analyses': []\n",
    "            }\n",
    "\n",
    "            for analysis in results['individual_analyses']:\n",
    "                serializable_analysis = {\n",
    "                    'article_title': analysis['article_title'],\n",
    "                    'article_url': analysis['article_url'],\n",
    "                    'impacts': [\n",
    "                        {\n",
    "                            'sector': impact.sector,\n",
    "                            'impact_percentage': impact.impact_percentage,\n",
    "                            'confidence_score': impact.confidence_score,\n",
    "                            'key_factors': impact.key_factors,\n",
    "                            'impact_type': impact.impact_type\n",
    "                        }\n",
    "                        for impact in analysis['impacts']\n",
    "                    ]\n",
    "                }\n",
    "                serializable_results['individual_analyses'].append(serializable_analysis)\n",
    "\n",
    "            with open(output_file, 'w', encoding='utf-8') as file:\n",
    "                json.dump(serializable_results, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "            print(f\"Results saved to {output_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n",
    "\n",
    "# Main execution function\n",
    "def analyze_news_articles(json_file_path: str, output_file: str = None):\n",
    "    \"\"\"\n",
    "    Main function to analyze news articles from JSON file\n",
    "\n",
    "    Args:\n",
    "        json_file_path (str): Path to your JSON file containing articles\n",
    "        output_file (str): Optional path to save results as JSON\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize analyzer\n",
    "    analyzer = NewsSectorAnalyzer()\n",
    "\n",
    "    # Load articles from JSON file\n",
    "    print(\"Loading articles from JSON file...\")\n",
    "    articles = analyzer.load_articles_from_json(json_file_path)\n",
    "\n",
    "    if not articles:\n",
    "        print(\"No articles found or failed to load articles\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(articles)} articles to analyze\")\n",
    "    print(\"Starting sector impact analysis...\\n\")\n",
    "\n",
    "    # Analyze all articles\n",
    "    results = analyzer.analyze_articles_batch(articles)\n",
    "\n",
    "    # Generate and print report\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING COMPREHENSIVE REPORT\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    report = analyzer.generate_report(results)\n",
    "    print(report)\n",
    "\n",
    "    # Save results to JSON if output file specified\n",
    "    if output_file:\n",
    "        analyzer.save_results_to_json(results, output_file)\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"\\n\\nSUMMARY STATISTICS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total articles analyzed: {results['total_articles_analyzed']}\")\n",
    "    print(f\"Total sectors identified: {len(results['sector_summary'])}\")\n",
    "\n",
    "    # Top 3 most impacted sectors\n",
    "    top_sectors = sorted(results['sector_summary'].items(),\n",
    "                        key=lambda x: x[1]['average_impact'], reverse=True)[:3]\n",
    "\n",
    "    print(f\"\\nTop 3 Most Impacted Sectors:\")\n",
    "    for i, (sector, stats) in enumerate(top_sectors, 1):\n",
    "        print(f\"{i}. {sector}: {stats['average_impact']}% avg impact\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage functions\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example usage - Replace 'your_articles.json' with your actual file path\n",
    "    \"\"\"\n",
    "    # Replace with your actual JSON file path\n",
    "    json_file_path = \"news_articles.json\"  # <-- CHANGE THIS TO YOUR FILE PATH\n",
    "    output_file = \"sector_analysis_results.json\"  # Optional: save results\n",
    "\n",
    "    # Analyze articles\n",
    "    results = analyze_news_articles(json_file_path, output_file)\n",
    "\n",
    "    if results:\n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "        print(f\"Check '{output_file}' for detailed JSON results\")\n",
    "\n",
    "# Alternative: Direct analysis function for custom usage\n",
    "def analyze_custom_articles():\n",
    "    \"\"\"\n",
    "    Custom analysis function - modify as needed\n",
    "    \"\"\"\n",
    "    # Option 1: Specify your JSON file path directly\n",
    "    file_path = input(\"news_articles.json\").strip()\n",
    "\n",
    "    # Option 2: Save results option\n",
    "    save_results = input(\"Save results to JSON file? (y/n): \").strip().lower()\n",
    "    output_file = None\n",
    "    if save_results == 'y':\n",
    "        output_file = input(\"final_analysis.json\").strip()\n",
    "\n",
    "    # Run analysis\n",
    "    analyze_news_articles(file_path, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"News Article Sector Impact Analyzer\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"1. Run with default settings (modify main() function)\")\n",
    "    print(\"2. Run with custom file path input\")\n",
    "\n",
    "    choice = input(\"Choose option (1 or 2): \").strip()\n",
    "\n",
    "    if choice == \"1\":\n",
    "        main()\n",
    "    elif choice == \"2\":\n",
    "        analyze_custom_articles()\n",
    "    else:\n",
    "        print(\"Invalid choice. Running default analysis...\")\n",
    "        main()"
   ],
   "id": "6c1217b381fcb39e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Article Sector Impact Analyzer\n",
      "==================================================\n",
      "1. Run with default settings (modify main() function)\n",
      "2. Run with custom file path input\n",
      "Loading articles from JSON file...\n",
      "Successfully loaded 100 articles from news_articles.json\n",
      "Found 100 articles to analyze\n",
      "Starting sector impact analysis...\n",
      "\n",
      "Analyzing article 1/100: NewsNews...\n",
      "Analyzing article 2/100: NewsNews...\n",
      "Analyzing article 3/100: US envoy rejects Hamas claim that it has agreed to...\n",
      "Analyzing article 4/100: BBC Sport...\n",
      "Analyzing article 5/100: Business...\n",
      "Analyzing article 6/100: Innovation...\n",
      "Analyzing article 7/100: Culture...\n",
      "Analyzing article 8/100: Arts...\n",
      "Analyzing article 9/100: Travel...\n",
      "Analyzing article 10/100: Earth...\n",
      "Analyzing article 11/100: Audio...\n",
      "Analyzing article 12/100: Video...\n",
      "Analyzing article 13/100: Live Now...\n",
      "Analyzing article 14/100: NewsNews...\n",
      "Analyzing article 15/100: NewsNews...\n",
      "Analyzing article 16/100: NewsNews...\n",
      "Analyzing article 17/100: NewsNews...\n",
      "Analyzing article 18/100: NewsNews...\n",
      "Analyzing article 19/100: NewsNews...\n",
      "Analyzing article 20/100: NewsNews...\n",
      "Analyzing article 21/100: NewsNews...\n",
      "Analyzing article 22/100: NewsNews...\n",
      "Analyzing article 23/100: NewsNews...\n",
      "Analyzing article 24/100: NewsNews...\n",
      "Analyzing article 25/100: NewsNews...\n",
      "Analyzing article 26/100: NewsNews...\n",
      "Analyzing article 27/100: BBC Homepage...\n",
      "Analyzing article 28/100: NewsNews...\n",
      "Analyzing article 29/100: NewsNews...\n",
      "Analyzing article 30/100: NewsNews...\n",
      "Analyzing article 31/100: NewsNews...\n",
      "Analyzing article 32/100: NewsNews...\n",
      "Analyzing article 33/100: NewsNews...\n",
      "Analyzing article 34/100: NewsNews...\n",
      "Analyzing article 35/100: NewsNews...\n",
      "Analyzing article 36/100: NewsNews...\n",
      "Analyzing article 37/100: NewsNews...\n",
      "Analyzing article 38/100: Executive Lounge...\n",
      "Analyzing article 39/100: Technology of Business...\n",
      "Analyzing article 40/100: Future of Business...\n",
      "Analyzing article 41/100: Technology...\n",
      "Analyzing article 42/100: Science & Health...\n",
      "Analyzing article 43/100: Artificial Intelligence...\n",
      "Analyzing article 44/100: AI v the Mind...\n",
      "Analyzing article 45/100: Film & TV...\n",
      "Analyzing article 46/100: Music...\n",
      "Analyzing article 47/100: Art & Design...\n",
      "Analyzing article 48/100: Style...\n",
      "Analyzing article 49/100: Books...\n",
      "Analyzing article 50/100: Entertainment & Arts...\n",
      "Analyzing article 51/100: Arts in Motion...\n",
      "Analyzing article 52/100: Destinations...\n",
      "Analyzing article 53/100: Destinations...\n",
      "Analyzing article 54/100: Destinations...\n",
      "Analyzing article 55/100: Destinations...\n",
      "Analyzing article 56/100: Destinations...\n",
      "Analyzing article 57/100: Destinations...\n",
      "Analyzing article 58/100: Destinations...\n",
      "Analyzing article 59/100: Destinations...\n",
      "Analyzing article 60/100: Destinations...\n",
      "Analyzing article 61/100: Destinations...\n",
      "Analyzing article 62/100: Destinations...\n",
      "Analyzing article 63/100: World’s Table...\n",
      "Analyzing article 64/100: Culture & Experiences...\n",
      "Analyzing article 65/100: Adventures...\n",
      "Analyzing article 66/100: The SpeciaList...\n",
      "Analyzing article 67/100: Natural Wonders...\n",
      "Analyzing article 68/100: Weather & Science...\n",
      "Analyzing article 69/100: Climate Solutions...\n",
      "Analyzing article 70/100: Sustainable Business...\n",
      "Analyzing article 71/100: Green Living...\n",
      "Analyzing article 72/100: Podcasts...\n",
      "Analyzing article 73/100: Radio Stations...\n",
      "Analyzing article 74/100: Live Now...\n",
      "Analyzing article 75/100: Recently Live...\n",
      "Analyzing article 76/100: BBC Weather...\n",
      "Analyzing article 77/100: Dozens injured after car ploughs into Liverpool cr...\n",
      "Analyzing article 78/100: What we know about the Liverpool FC parade inciden...\n",
      "Analyzing article 79/100: Greek coastguards charged over 2023 migrant shipwr...\n",
      "Analyzing article 80/100: India's ex-wrestling chief cleared in minor's sexu...\n",
      "Analyzing article 81/100: What you need to know ahead of South Korea's snap ...\n",
      "Analyzing article 82/100: Moment car drives into crowd at Liverpool trophy p...\n",
      "Analyzing article 83/100: North Korea says US 'Golden Dome' risks 'space nuc...\n",
      "Analyzing article 84/100: Nepal's 'Everest Man' sets record with 31st summit...\n",
      "Analyzing article 85/100: From prodigy to leader: Can Shubman Gill shape the...\n",
      "Analyzing article 86/100: King prepares to give key speech backing Canada...\n",
      "Analyzing article 87/100: Chinese-owned Volvo Cars to cut 3,000 jobs...\n",
      "Analyzing article 88/100: BBC given rare access to search for Hezbollah posi...\n",
      "Analyzing article 89/100: Far-right marchers attack Palestinians as Israel m...\n",
      "Analyzing article 90/100: Could Nigeria's careful ethnic balancing act be un...\n",
      "Analyzing article 91/100: An Indian teacher was killed - then he got falsely...\n",
      "Analyzing article 92/100: 'My life was saved by a stranger on the other side...\n",
      "Analyzing article 93/100: The people who think AI might become conscious...\n",
      "Analyzing article 94/100: China student says college made her 'take off trou...\n",
      "Analyzing article 95/100: Humanoid robots fight in Chinese kick-boxing compe...\n",
      "Analyzing article 96/100: Watch: Push in the face by wife was joke not domes...\n",
      "Analyzing article 97/100: Moment truck explodes while driving through Chicag...\n",
      "Analyzing article 98/100: North Korea arrests senior official over warship l...\n",
      "Analyzing article 99/100: Fake discounts on Shein 'breach law', EU says...\n",
      "Analyzing article 100/100: 'Nowhere is safe' - Cameroonians trapped between s...\n",
      "\n",
      "================================================================================\n",
      "GENERATING COMPREHENSIVE REPORT\n",
      "================================================================================\n",
      "================================================================================\n",
      "NEWS ARTICLE SECTOR IMPACT ANALYSIS REPORT\n",
      "================================================================================\n",
      "\n",
      "Total Articles Analyzed: 100\n",
      "\n",
      "SECTOR IMPACT SUMMARY\n",
      "----------------------------------------\n",
      "\n",
      "E-commerce/Online Retail:\n",
      "  Average Impact: 16.2%\n",
      "  Maximum Impact: 26.42%\n",
      "  Frequency: 4 articles\n",
      "  Coverage: 4.0% of all articles\n",
      "\n",
      "Energy:\n",
      "  Average Impact: 14.57%\n",
      "  Maximum Impact: 25.34%\n",
      "  Frequency: 16 articles\n",
      "  Coverage: 16.0% of all articles\n",
      "\n",
      "Technology/Digital Platforms:\n",
      "  Average Impact: 14.18%\n",
      "  Maximum Impact: 20.59%\n",
      "  Frequency: 14 articles\n",
      "  Coverage: 14.0% of all articles\n",
      "\n",
      "Healthcare:\n",
      "  Average Impact: 12.93%\n",
      "  Maximum Impact: 21.03%\n",
      "  Frequency: 15 articles\n",
      "  Coverage: 15.0% of all articles\n",
      "\n",
      "Fashion/Apparel:\n",
      "  Average Impact: 12.55%\n",
      "  Maximum Impact: 19.89%\n",
      "  Frequency: 15 articles\n",
      "  Coverage: 15.0% of all articles\n",
      "\n",
      "Financial Services:\n",
      "  Average Impact: 11.55%\n",
      "  Maximum Impact: 11.55%\n",
      "  Frequency: 1 articles\n",
      "  Coverage: 1.0% of all articles\n",
      "\n",
      "Agriculture/Food:\n",
      "  Average Impact: 10.98%\n",
      "  Maximum Impact: 22.4%\n",
      "  Frequency: 14 articles\n",
      "  Coverage: 14.0% of all articles\n",
      "\n",
      "Real Estate:\n",
      "  Average Impact: 10.37%\n",
      "  Maximum Impact: 14.59%\n",
      "  Frequency: 11 articles\n",
      "  Coverage: 11.0% of all articles\n",
      "\n",
      "Transportation/Logistics:\n",
      "  Average Impact: 10.32%\n",
      "  Maximum Impact: 10.32%\n",
      "  Frequency: 1 articles\n",
      "  Coverage: 1.0% of all articles\n",
      "\n",
      "\n",
      "TOP INDIVIDUAL ARTICLE IMPACTS\n",
      "----------------------------------------\n",
      "\n",
      "1. Fake discounts on Shein 'breach law', EU says...\n",
      "   Sector: E-commerce/Online Retail\n",
      "   Impact: 26.42% (negative)\n",
      "   Confidence: 0.581\n",
      "\n",
      "2. Climate Solutions...\n",
      "   Sector: Energy\n",
      "   Impact: 25.34% (negative)\n",
      "   Confidence: 0.534\n",
      "\n",
      "3. Climate Solutions...\n",
      "   Sector: Agriculture/Food\n",
      "   Impact: 22.4% (negative)\n",
      "   Confidence: 0.47\n",
      "\n",
      "4. Earth...\n",
      "   Sector: Energy\n",
      "   Impact: 21.99% (positive)\n",
      "   Confidence: 0.487\n",
      "\n",
      "5. BBC given rare access to search for Hezbollah positions with...\n",
      "   Sector: Healthcare\n",
      "   Impact: 21.03% (negative)\n",
      "   Confidence: 0.447\n",
      "\n",
      "6. Technology...\n",
      "   Sector: Technology/Digital Platforms\n",
      "   Impact: 20.59% (neutral)\n",
      "   Confidence: 0.465\n",
      "\n",
      "7. Science & Health...\n",
      "   Sector: Healthcare\n",
      "   Impact: 20.31% (positive)\n",
      "   Confidence: 0.458\n",
      "\n",
      "8. Video...\n",
      "   Sector: Energy\n",
      "   Impact: 19.98% (neutral)\n",
      "   Confidence: 0.45\n",
      "\n",
      "9. Artificial Intelligence...\n",
      "   Sector: Fashion/Apparel\n",
      "   Impact: 19.89% (positive)\n",
      "   Confidence: 0.448\n",
      "\n",
      "10. Green Living...\n",
      "   Sector: Energy\n",
      "   Impact: 19.46% (negative)\n",
      "   Confidence: 0.412\n",
      "Results saved to sector_analysis_results.json\n",
      "\n",
      "\n",
      "SUMMARY STATISTICS\n",
      "----------------------------------------\n",
      "Total articles analyzed: 100\n",
      "Total sectors identified: 9\n",
      "\n",
      "Top 3 Most Impacted Sectors:\n",
      "1. E-commerce/Online Retail: 16.2% avg impact\n",
      "2. Energy: 14.57% avg impact\n",
      "3. Technology/Digital Platforms: 14.18% avg impact\n",
      "\n",
      "Analysis completed successfully!\n",
      "Check 'sector_analysis_results.json' for detailed JSON results\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T10:26:07.010364Z",
     "start_time": "2025-05-27T10:25:55.909582Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install sklearn",
   "id": "4b9549ef466aa07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "# Configuration\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = 30522\n",
    "    hidden_size: int = 768\n",
    "    num_attention_heads: int = 12\n",
    "    num_hidden_layers: int = 6\n",
    "    intermediate_size: int = 3072\n",
    "    max_position_embeddings: int = 512\n",
    "    dropout_prob: float = 0.1\n",
    "    num_sectors: int = 8\n",
    "    lora_rank: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.1\n",
    "\n",
    "# LoRA Layer for Parameter-Efficient Fine-tuning\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, rank: int = 16, alpha: int = 32, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "        # LoRA parameters\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Original layer (frozen during LoRA training)\n",
    "        self.original_layer = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Original forward pass\n",
    "        original_output = self.original_layer(x)\n",
    "\n",
    "        # LoRA forward pass\n",
    "        lora_output = self.dropout(x) @ self.lora_A @ self.lora_B * self.scaling\n",
    "\n",
    "        return original_output + lora_output\n",
    "\n",
    "    def freeze_original(self):\n",
    "        \"\"\"Freeze original layer parameters for LoRA training\"\"\"\n",
    "        for param in self.original_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Multi-Head Attention with LoRA\n",
    "class MultiHeadAttentionLoRA(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "        # LoRA layers for Q, K, V projections\n",
    "        self.query = LoRALayer(config.hidden_size, config.hidden_size, config.lora_rank, config.lora_alpha)\n",
    "        self.key = LoRALayer(config.hidden_size, config.hidden_size, config.lora_rank, config.lora_alpha)\n",
    "        self.value = LoRALayer(config.hidden_size, config.hidden_size, config.lora_rank, config.lora_alpha)\n",
    "        self.output = LoRALayer(config.hidden_size, config.hidden_size, config.lora_rank, config.lora_alpha)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "\n",
    "        # Generate Q, K, V\n",
    "        Q = self.query(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_scores += attention_mask\n",
    "\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention_probs, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Output projection\n",
    "        output = self.output(context)\n",
    "        return output, attention_probs\n",
    "\n",
    "# Feed Forward Network with LoRA\n",
    "class FeedForwardLoRA(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.dense1 = LoRALayer(config.hidden_size, config.intermediate_size, config.lora_rank, config.lora_alpha)\n",
    "        self.dense2 = LoRALayer(config.intermediate_size, config.hidden_size, config.lora_rank, config.lora_alpha)\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense1(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.dense2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "# Transformer Layer\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttentionLoRA(config)\n",
    "        self.feed_forward = FeedForwardLoRA(config)\n",
    "        self.norm1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attention_output, attention_probs = self.attention(self.norm1(hidden_states), attention_mask)\n",
    "        hidden_states = hidden_states + self.dropout(attention_output)\n",
    "\n",
    "        # Feed forward with residual connection\n",
    "        ff_output = self.feed_forward(self.norm2(hidden_states))\n",
    "        hidden_states = hidden_states + self.dropout(ff_output)\n",
    "\n",
    "        return hidden_states, attention_probs\n",
    "\n",
    "# News Sector Classification Model\n",
    "class NewsSectorModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.norm = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "\n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(config) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "        # Classification heads\n",
    "        self.sector_classifier = nn.Linear(config.hidden_size, config.num_sectors)\n",
    "        self.confidence_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.dropout_prob),\n",
    "            nn.Linear(config.hidden_size // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Temperature scaling for calibration\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, std=0.02)\n",
    "\n",
    "    def freeze_for_lora(self):\n",
    "        \"\"\"Freeze all parameters except LoRA parameters\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'lora_' not in name and 'sector_classifier' not in name and 'confidence_head' not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Freeze original layers in LoRA components\n",
    "        for layer in self.layers:\n",
    "            layer.attention.query.freeze_original()\n",
    "            layer.attention.key.freeze_original()\n",
    "            layer.attention.value.freeze_original()\n",
    "            layer.attention.output.freeze_original()\n",
    "            layer.feed_forward.dense1.freeze_original()\n",
    "            layer.feed_forward.dense2.freeze_original()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, return_attention=False):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        # Create position ids\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device).expand((batch_size, -1))\n",
    "\n",
    "        # Embeddings\n",
    "        token_embeds = self.token_embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        hidden_states = self.norm(token_embeds + position_embeds)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # Create attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask[:, None, None, :] * -10000.0\n",
    "\n",
    "        # Pass through transformer layers\n",
    "        all_attention_probs = []\n",
    "        for layer in self.layers:\n",
    "            hidden_states, attention_probs = layer(hidden_states, attention_mask)\n",
    "            if return_attention:\n",
    "                all_attention_probs.append(attention_probs)\n",
    "\n",
    "        # Global average pooling for classification\n",
    "        if attention_mask is not None:\n",
    "            mask = (attention_mask.squeeze() != -10000.0).float().unsqueeze(-1)\n",
    "            pooled = (hidden_states * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        else:\n",
    "            pooled = hidden_states.mean(dim=1)\n",
    "\n",
    "        # Classification outputs\n",
    "        sector_logits = self.sector_classifier(pooled)\n",
    "        confidence_score = self.confidence_head(pooled)\n",
    "\n",
    "        # Temperature scaling for calibration\n",
    "        calibrated_logits = sector_logits / self.temperature\n",
    "\n",
    "        outputs = {\n",
    "            'sector_logits': calibrated_logits,\n",
    "            'confidence_score': confidence_score,\n",
    "            'pooled_output': pooled\n",
    "        }\n",
    "\n",
    "        if return_attention:\n",
    "            outputs['attention_probs'] = all_attention_probs\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Dataset for News Articles\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Training utilities\n",
    "class NewsClassificationTrainer:\n",
    "    def __init__(self, model, config, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.sector_names = [\n",
    "            'Technology', 'Finance', 'Healthcare', 'Energy',\n",
    "            'Real Estate', 'Consumer', 'Industrial', 'Other'\n",
    "        ]\n",
    "\n",
    "    def train_epoch(self, dataloader, optimizer, criterion):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(input_ids, attention_mask)\n",
    "\n",
    "            # Classification loss\n",
    "            classification_loss = criterion(outputs['sector_logits'], labels)\n",
    "\n",
    "            # Confidence regularization loss\n",
    "            confidence_loss = torch.mean((outputs['confidence_score'] - 0.5) ** 2)\n",
    "\n",
    "            total_loss_batch = classification_loss + 0.1 * confidence_loss\n",
    "            total_loss_batch.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += total_loss_batch.item()\n",
    "            _, predicted = torch.max(outputs['sector_logits'], 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        return total_loss / len(dataloader), correct / total\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_predictions = []\n",
    "        all_confidences = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "\n",
    "                # Get predictions and confidences\n",
    "                probabilities = F.softmax(outputs['sector_logits'], dim=-1)\n",
    "                _, predicted = torch.max(outputs['sector_logits'], 1)\n",
    "\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_confidences.extend(outputs['confidence_score'].cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return accuracy, all_predictions, all_confidences, all_labels\n",
    "\n",
    "    def predict_single(self, text: str, tokenizer) -> Dict:\n",
    "        \"\"\"Predict sector and confidence for a single news article\"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        # Tokenize input\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.config.max_position_embeddings,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask, return_attention=True)\n",
    "\n",
    "            # Get sector probabilities\n",
    "            probabilities = F.softmax(outputs['sector_logits'], dim=-1)\n",
    "            confidence = outputs['confidence_score'].item()\n",
    "\n",
    "            # Get predictions\n",
    "            predicted_sector_idx = torch.argmax(probabilities, dim=-1).item()\n",
    "            sector_percentages = probabilities.cpu().numpy()[0]\n",
    "\n",
    "            result = {\n",
    "                'predicted_sector': self.sector_names[predicted_sector_idx],\n",
    "                'confidence_score': confidence,\n",
    "                'sector_percentages': {\n",
    "                    sector: float(percentage)\n",
    "                    for sector, percentage in zip(self.sector_names, sector_percentages)\n",
    "                },\n",
    "                'top_3_sectors': [\n",
    "                    {\n",
    "                        'sector': self.sector_names[idx],\n",
    "                        'percentage': float(sector_percentages[idx])\n",
    "                    }\n",
    "                    for idx in np.argsort(sector_percentages)[-3:][::-1]\n",
    "                ]\n",
    "            }\n",
    "\n",
    "        return result\n",
    "\n",
    "# Example usage and training setup\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample news data for demonstration\"\"\"\n",
    "    sample_texts = [\n",
    "        \"Apple announces new iPhone with advanced AI capabilities and improved camera technology\",\n",
    "        \"Federal Reserve raises interest rates by 0.25% to combat inflation concerns\",\n",
    "        \"New cancer treatment shows promising results in clinical trials\",\n",
    "        \"Oil prices surge amid geopolitical tensions in the Middle East\",\n",
    "        \"Housing market shows signs of cooling as mortgage rates climb\",\n",
    "        \"Consumer spending drops as inflation impacts household budgets\",\n",
    "        \"Manufacturing output increases for third consecutive month\",\n",
    "        \"Local weather forecast predicts sunny skies for the weekend\"\n",
    "    ]\n",
    "\n",
    "    sample_labels = [0, 1, 2, 3, 4, 5, 6, 7]  # Technology, Finance, Healthcare, Energy, Real Estate, Consumer, Industrial, Other\n",
    "\n",
    "    return sample_texts, sample_labels\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training and demonstration function\"\"\"\n",
    "    # Initialize configuration\n",
    "    config = ModelConfig()\n",
    "\n",
    "    # Create model\n",
    "    model = NewsSectorModel(config)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = NewsClassificationTrainer(model, config)\n",
    "\n",
    "    # Create sample data\n",
    "    texts, labels = create_sample_data()\n",
    "\n",
    "    print(\"News Sector Classification Model initialized successfully!\")\n",
    "    print(f\"Model has {sum(p.numel() for p in model.parameters())} total parameters\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    # Demonstrate LoRA fine-tuning setup\n",
    "    model.freeze_for_lora()\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Parameters after LoRA freeze: {trainable_params} ({trainable_params/sum(p.numel() for p in model.parameters())*100:.2f}%)\")\n",
    "\n",
    "    # Example prediction (requires tokenizer - using placeholder)\n",
    "    sample_text = \"Tech giant announces breakthrough in quantum computing research\"\n",
    "    print(f\"\\nSample prediction for: '{sample_text}'\")\n",
    "    print(\"Note: Full prediction requires proper tokenizer initialization\")\n",
    "\n",
    "    return model, trainer, config\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, trainer, config = main()"
   ],
   "id": "519d280777bf6b9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T13:25:39.243414Z",
     "start_time": "2025-05-27T13:25:37.625056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Initialize NLP components\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Define sectors (GICS classification)\n",
    "SECTORS = [\n",
    "    \"Energy\", \"Materials\", \"Industrials\", \"Consumer Discretionary\",\n",
    "    \"Consumer Staples\", \"Health Care\", \"Financials\", \"Information Technology\",\n",
    "    \"Communication Services\", \"Utilities\", \"Real Estate\", \"Transportation\"\n",
    "]\n",
    "\n",
    "def load_articles(json_file):\n",
    "    \"\"\"Load articles from JSON file\"\"\"\n",
    "    if not os.path.isfile(json_file):\n",
    "        print(f\"ERROR: Input file '{json_file}' does not exist.\")\n",
    "        sys.exit(1)\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "    return data\n",
    "\n",
    "def analyze_article(article):\n",
    "    \"\"\"Analyze a single article\"\"\"\n",
    "    content = article.get('content')\n",
    "    if not content:\n",
    "        print(f\"Skipping article without content: {article.get('title', 'No title')}\")\n",
    "        return None\n",
    "\n",
    "    doc = nlp(content)\n",
    "\n",
    "    # Sector classification\n",
    "    classification = classifier(\n",
    "        content[:1000],  # Truncate to avoid max length issues\n",
    "        candidate_labels=SECTORS,\n",
    "        multi_label=True\n",
    "    )\n",
    "\n",
    "    # Sentiment analysis (simple version)\n",
    "    positive_words = [\"growth\", \"rise\", \"increase\", \"profit\", \"gain\"]\n",
    "    negative_words = [\"fall\", \"decline\", \"loss\", \"drop\", \"explosion\", \"crash\"]\n",
    "    sentiment_score = 0\n",
    "    for token in doc:\n",
    "        if token.text.lower() in positive_words:\n",
    "            sentiment_score += 1\n",
    "        elif token.text.lower() in negative_words:\n",
    "            sentiment_score -= 1\n",
    "\n",
    "    # Extract entities\n",
    "    entities = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['ORG', 'PRODUCT', 'GPE']:\n",
    "            entities.add(ent.text)\n",
    "\n",
    "    # Parse date safely\n",
    "    date_str = article.get('date')\n",
    "    if not date_str:\n",
    "        print(f\"Skipping article without date: {article.get('title', 'No title')}\")\n",
    "        return None\n",
    "    try:\n",
    "        date_parsed = datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping article with invalid date '{date_str}': {article.get('title', 'No title')}\")\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'date': date_parsed,\n",
    "        'top_sector': classification['labels'][0],\n",
    "        'sector_confidence': classification['scores'][0],\n",
    "        'sentiment': sentiment_score,\n",
    "        'entities': list(entities),\n",
    "        'title': article.get('title', 'No title')\n",
    "    }\n",
    "\n",
    "def calculate_growth_metrics(analyzed_articles):\n",
    "    \"\"\"Calculate growth metrics by sector\"\"\"\n",
    "    sector_data = defaultdict(lambda: {\n",
    "        'count': 0,\n",
    "        'sentiments': [],\n",
    "        'entities': set(),\n",
    "        'titles': []\n",
    "    })\n",
    "\n",
    "    for article in analyzed_articles:\n",
    "        sector = article['top_sector']\n",
    "        sector_data[sector]['count'] += 1\n",
    "        sector_data[sector]['sentiments'].append(article['sentiment'])\n",
    "        sector_data[sector]['entities'].update(article['entities'])\n",
    "        sector_data[sector]['titles'].append(article['title'])\n",
    "\n",
    "    # Calculate time-based growth (last 3 days vs previous 3 days)\n",
    "    now = datetime.now()\n",
    "    recent_cutoff = now - pd.Timedelta(days=3)\n",
    "\n",
    "    results = []\n",
    "    for sector, data in sector_data.items():\n",
    "        # Time-based analysis\n",
    "        recent_count = sum(1 for article in analyzed_articles\n",
    "                         if article['top_sector'] == sector\n",
    "                         and article['date'] >= recent_cutoff)\n",
    "        previous_count = data['count'] - recent_count\n",
    "\n",
    "        # Growth rate calculation\n",
    "        growth_rate = ((recent_count - previous_count) / (previous_count + 0.001)) * 100\n",
    "\n",
    "        # Sentiment analysis\n",
    "        avg_sentiment = sum(data['sentiments']) / len(data['sentiments']) if data['sentiments'] else 0\n",
    "\n",
    "        # Entity diversity\n",
    "        entity_diversity = len(data['entities'])\n",
    "\n",
    "        # Recent article titles (for context)\n",
    "        recent_titles = [title for article in analyzed_articles\n",
    "                        if article['top_sector'] == sector\n",
    "                        and article['date'] >= recent_cutoff\n",
    "                        for title in [article['title']]]\n",
    "\n",
    "        results.append({\n",
    "            'Sector': sector,\n",
    "            'Total Articles': data['count'],\n",
    "            'Growth Rate (%)': round(growth_rate, 2),\n",
    "            'Avg Sentiment': round(avg_sentiment, 2),\n",
    "            'Unique Entities': entity_diversity,\n",
    "            'Recent Titles': recent_titles[:3]  # Show top 3 recent titles\n",
    "        })\n",
    "\n",
    "    return sorted(results, key=lambda x: x['Growth Rate (%)'], reverse=True)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Analyze news articles for sector growth')\n",
    "    parser.add_argument('input_file', help='news_articles.json')\n",
    "    parser.add_argument('--output', help='results.json', default='sector_growth.csv')\n",
    "    args, unknown = parser.parse_known_args()  # safe for Jupyter\n",
    "\n",
    "    print(f\"Loading articles from {args.input_file}...\")\n",
    "    articles = load_articles(args.input_file)\n",
    "    if not articles:\n",
    "        print(\"No articles loaded. Please check your input file.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"Loaded {len(articles)} articles\")\n",
    "    print(f\"Sample article keys: {list(articles[0].keys())}\")\n",
    "\n",
    "    print(\"Analyzing articles...\")\n",
    "    analyzed_articles = [analyze_article(article) for article in articles]\n",
    "    analyzed_articles = [a for a in analyzed_articles if a is not None]\n",
    "\n",
    "    if not analyzed_articles:\n",
    "        print(\"No valid articles to analyze after filtering. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"Calculating growth metrics...\")\n",
    "    growth_analysis = calculate_growth_metrics(analyzed_articles)\n",
    "\n",
    "    # Save results\n",
    "    df = pd.DataFrame(growth_analysis)\n",
    "    if args.output.endswith('.json'):\n",
    "        df.to_json(args.output, orient='records', indent=2)\n",
    "    else:\n",
    "        df.to_csv(args.output, index=False)\n",
    "\n",
    "    print(f\"Analysis complete. Results saved to {args.output}\")\n",
    "    print(\"\\nTop Growing Sectors:\")\n",
    "    print(df.head(5).to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "b1801766efc73757",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading articles from C:\\Users\\Krinal\\AppData\\Roaming\\jupyter\\runtime\\kernel-bda4135e-b98c-406e-8cf2-736b3cf48b30.json...\n",
      "Loaded 1 articles\n",
      "Sample article keys: ['shell_port', 'iopub_port', 'stdin_port', 'control_port', 'hb_port', 'ip', 'key', 'transport', 'signature_scheme', 'kernel_name', 'jupyter_session']\n",
      "Analyzing articles...\n",
      "Skipping article without content: No title\n",
      "No valid articles to analyze after filtering. Exiting.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[31mSystemExit\u001B[39m\u001B[31m:\u001B[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krinal\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T13:18:15.879946Z",
     "start_time": "2025-05-27T13:18:11.555110Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install hf_xet",
   "id": "3bf55536bc682884",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-win_amd64.whl.metadata (883 bytes)\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 14.4 MB/s eta 0:00:00\n",
      "Installing collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.1.2\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T11:02:25.772052Z",
     "start_time": "2025-05-28T11:02:03.861152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Load English language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "class SectorAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Define sector keywords and related terms\n",
    "        self.sector_keywords = {\n",
    "            \"Energy\": [\"oil\", \"gas\", \"renewable\", \"solar\", \"wind\", \"energy\", \"petroleum\", \"OPEC\", \"drilling\"],\n",
    "            \"Technology\": [\"tech\", \"software\", \"AI\", \"artificial intelligence\", \"chip\", \"semiconductor\", \"cloud\"],\n",
    "            \"Healthcare\": [\"pharma\", \"hospital\", \"medical\", \"biotech\", \"vaccine\", \"FDA\", \"healthcare\"],\n",
    "            \"Finance\": [\"bank\", \"investment\", \"stock\", \"market\", \"crypto\", \"bitcoin\", \"interest rate\"],\n",
    "            \"Manufacturing\": [\"factory\", \"manufacturing\", \"production\", \"supply chain\", \"automobile\"],\n",
    "            \"Retail\": [\"retail\", \"e-commerce\", \"amazon\", \"walmart\", \"consumer\", \"shopping\"],\n",
    "            \"Transportation\": [\"airline\", \"shipping\", \"logistics\", \"trucking\", \"aviation\"],\n",
    "            \"Real Estate\": [\"housing\", \"real estate\", \"mortgage\", \"property\", \"REIT\"],\n",
    "            \"Agriculture\": [\"farm\", \"agriculture\", \"crop\", \"grain\", \"livestock\"]\n",
    "        }\n",
    "\n",
    "        # Initialize phrase matcher\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        for sector, patterns in self.sector_keywords.items():\n",
    "            patterns = [nlp(text) for text in patterns]\n",
    "            self.matcher.add(sector, None, *patterns)\n",
    "\n",
    "    def identify_sectors(self, text):\n",
    "        doc = nlp(text)\n",
    "        matches = self.matcher(doc)\n",
    "\n",
    "        sector_counts = defaultdict(int)\n",
    "        for match_id, start, end in matches:\n",
    "            sector = nlp.vocab.strings[match_id]\n",
    "            sector_counts[sector] += 1\n",
    "\n",
    "        return sector_counts\n",
    "\n",
    "    def predict_growth(self, sector_counts, article_date, text=None):\n",
    "    # Base growth rates (would normally come from economic models)\n",
    "        base_growth = {\n",
    "            \"Energy\": {\"short\": 2.5, \"long\": 15},\n",
    "            \"Technology\": {\"short\": 8, \"long\": 45},\n",
    "            \"Healthcare\": {\"short\": 5, \"long\": 30},\n",
    "            \"Finance\": {\"short\": 3, \"long\": 20},\n",
    "            \"Manufacturing\": {\"short\": 2, \"long\": 12},\n",
    "            \"Retail\": {\"short\": 4, \"long\": 18},\n",
    "            \"Transportation\": {\"short\": 3.5, \"long\": 22},\n",
    "            \"Real Estate\": {\"short\": 1.5, \"long\": 10},\n",
    "            \"Agriculture\": {\"short\": 2, \"long\": 15}\n",
    "        }\n",
    "\n",
    "        # Initialize sentiment factor (default neutral)\n",
    "        sentiment_factor = 0\n",
    "        if text:  # Only analyze sentiment if text is provided\n",
    "            sentiment_factor = self.analyze_sentiment(text)\n",
    "\n",
    "        results = {}\n",
    "        for sector, count in sector_counts.items():\n",
    "            if count > 1:  # Only consider sectors with multiple mentions\n",
    "                base = base_growth.get(sector, {\"short\": 2, \"long\": 10})\n",
    "                adjusted_short = base[\"short\"] * (1 + sentiment_factor*0.1)\n",
    "                adjusted_long = base[\"long\"] * (1 + sentiment_factor*0.2)\n",
    "                results[sector] = {\n",
    "                    \"confidence\": min(100, count*15),  # Scale with mention count\n",
    "                    \"short_term_growth\": round(adjusted_short, 1),\n",
    "                    \"long_term_growth\": round(adjusted_long, 1),\n",
    "                    \"trend_indicator\": \"↑\" if sentiment_factor > 0 else \"↓\" if sentiment_factor < 0 else \"→\"\n",
    "                }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"Simple sentiment analysis (would use better model in production)\"\"\"\n",
    "        doc = nlp(text)\n",
    "        positive_words = [\"growth\", \"boom\", \"rise\", \"increase\", \"surge\"]\n",
    "        negative_words = [\"decline\", \"fall\", \"drop\", \"crisis\", \"slump\"]\n",
    "\n",
    "        score = 0\n",
    "        for token in doc:\n",
    "            if token.text.lower() in positive_words:\n",
    "                score += 1\n",
    "            elif token.text.lower() in negative_words:\n",
    "                score -= 1\n",
    "\n",
    "        return score / len(doc) * 100 if doc else 0\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = SectorAnalyzer()\n",
    "\n",
    "    # Load your JSON data\n",
    "    with open('NewsArticles.json', 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    sector_report = defaultdict(list)\n",
    "# In your main processing loop:\n",
    "    for article in articles:\n",
    "        article_text = article[\"title\"] + \" \" + article[\"content\"]\n",
    "        sectors = analyzer.identify_sectors(article_text)\n",
    "        growth_predictions = analyzer.predict_growth(\n",
    "            sectors,\n",
    "            article[\"date\"],\n",
    "            text=article_text  # Pass the text for sentiment analysis\n",
    "        )\n",
    "\n",
    "        if growth_predictions:\n",
    "            primary_sector = max(growth_predictions.items(), key=lambda x: x[1][\"confidence\"])[0]\n",
    "            sector_report[primary_sector].append({\n",
    "                \"article_title\": article[\"title\"],\n",
    "                \"url\": article[\"url\"],\n",
    "                \"confidence\": growth_predictions[primary_sector][\"confidence\"],\n",
    "                \"short_term\": growth_predictions[primary_sector][\"short_term_growth\"],\n",
    "                \"long_term\": growth_predictions[primary_sector][\"long_term_growth\"],\n",
    "                \"trend\": growth_predictions[primary_sector][\"trend_indicator\"]\n",
    "            })\n",
    "\n",
    "    # Generate sector growth summary\n",
    "    summary = {}\n",
    "    for sector, articles in sector_report.items():\n",
    "        avg_short = sum(a[\"short_term\"] for a in articles) / len(articles)\n",
    "        avg_long = sum(a[\"long_term\"] for a in articles) / len(articles)\n",
    "        summary[sector] = {\n",
    "            \"article_count\": len(articles),\n",
    "            \"average_short_term_growth\": round(avg_short, 1),\n",
    "            \"average_long_term_growth\": round(avg_long, 1),\n",
    "            \"sample_articles\": [a[\"article_title\"] for a in articles[:3]]\n",
    "        }\n",
    "\n",
    "    print(\"\\nSector Growth Potential Summary:\")\n",
    "    print(pd.DataFrame.from_dict(summary, orient=\"index\"))\n",
    "\n",
    "    # Save detailed report\n",
    "    with open(\"SectorAnalysisReport.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"summary\": summary,\n",
    "            \"detailed_analysis\": sector_report\n",
    "        }, f, indent=2)"
   ],
   "id": "810cdff54659b378",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sector Growth Potential Summary:\n",
      "               article_count  average_short_term_growth  \\\n",
      "Technology                 1                        8.0   \n",
      "Manufacturing              1                        2.0   \n",
      "Energy                     1                        2.5   \n",
      "Finance                    2                        3.0   \n",
      "\n",
      "               average_long_term_growth  \\\n",
      "Technology                         45.0   \n",
      "Manufacturing                      12.4   \n",
      "Energy                             14.7   \n",
      "Finance                            20.0   \n",
      "\n",
      "                                                 sample_articles  \n",
      "Technology                               [Taiwan News & Opinion]  \n",
      "Manufacturing                              [Hong Kong Originals]  \n",
      "Energy                                                 [Opinion]  \n",
      "Finance        [HKFP Lens, Hong Kong Free Press Transparency ...  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:00:28.320335Z",
     "start_time": "2025-05-28T11:25:40.195075Z"
    }
   },
   "cell_type": "code",
   "source": [
    " import json\n",
    "from transformers import pipeline\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load Zero-Shot classifier\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Define your sectors\n",
    "sectors = [\n",
    "    \"Disasters & Accidents\",\n",
    "    \"Politics & Government\",\n",
    "    \"Entertainment & Celebrity\",\n",
    "    \"Technology\",\n",
    "    \"Health\",\n",
    "    \"Energy & Environment\",\n",
    "    \"Business & Finance\",\n",
    "    \"Religion\",\n",
    "    \"Social Issues\"\n",
    "]\n",
    "\n",
    "def classify_article(text, confidence_threshold=0.7):\n",
    "    \"\"\"Classify article text into sectors\"\"\"\n",
    "    result = classifier(text, sectors, multi_label=True)\n",
    "    return [\n",
    "        sector for sector, score in zip(result[\"labels\"], result[\"scores\"])\n",
    "        if score > confidence_threshold\n",
    "    ]\n",
    "\n",
    "# Load your JSON data\n",
    "with open('NewsArticles.json', 'r', encoding='utf-8') as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "# Categorize articles\n",
    "sector_articles = defaultdict(list)\n",
    "\n",
    "for article in articles:\n",
    "    # Combine title and content for better classification\n",
    "    text = f\"{article['title']}. {article['content']}\"\n",
    "\n",
    "    # Get relevant sectors\n",
    "    article_sectors = classify_article(text)\n",
    "\n",
    "    # Add to each relevant sector bucket\n",
    "    for sector in article_sectors:\n",
    "        sector_articles[sector].append(article)\n",
    "\n",
    "# Save results\n",
    "with open('categorized_articles.json', 'w') as f:\n",
    "    json.dump(sector_articles, f, indent=2)\n",
    "\n",
    "print(\"Categorization complete. Results saved to categorized_articles.json\")"
   ],
   "id": "dc99f98383d9b7cb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorization complete. Results saved to categorized_articles.json\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
