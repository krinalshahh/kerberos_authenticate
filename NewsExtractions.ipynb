{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-02T06:42:00.761437Z",
     "start_time": "2025-06-02T06:41:18.936403Z"
    }
   },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "class NewsCrawler:\n",
    "    def __init__(self, starting_url, no_of_articles, depth, delay):\n",
    "        self.starting_url = starting_url\n",
    "        self.no_of_articles = no_of_articles\n",
    "        self.depth = depth\n",
    "        self.delay = delay\n",
    "        self.urls_visited = set()\n",
    "        self.queue = deque()\n",
    "        self.ua = UserAgent()\n",
    "        self.articles = []\n",
    "        self.session = requests.Session()\n",
    "        self.domain = urlparse(self.starting_url).netloc\n",
    "\n",
    "        # session headers\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': self.ua.random,\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Sec-Fetch-User': '?1',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "        })\n",
    "\n",
    "    def is_url_valid(self, url):\n",
    "        parsed_url = urlparse(url)\n",
    "        return (parsed_url.netloc == self.domain\n",
    "                and url not in self.urls_visited\n",
    "                and not any(extension in url.lower()\n",
    "                           for extension in ['.pdf', '.jpg', '.png', 'jpeg', '.gif', '.mp4']))\n",
    "\n",
    "    def is_page_an_article(self, url, soup):\n",
    "        # Specific checks for Hong Kong Business site\n",
    "        article = soup.find('article') or soup.find('div', {'class': 'article-content'})\n",
    "        headline = soup.find('h1', {'class': 'article-title'}) or soup.find('h1')\n",
    "\n",
    "        # Check for date - Hong Kong Business uses 'date' class\n",
    "        date_published = soup.find('div', class_='date') or soup.find('time') or soup.find('meta', property='article:published_time')\n",
    "\n",
    "        return (article is not None and headline is not None and date_published is not None)\n",
    "\n",
    "    def extract_found_article(self, soup):\n",
    "        # Try to find the main article content\n",
    "        article = (soup.find('article') or\n",
    "                  soup.find('div', {'class': 'article-content'}) or\n",
    "                  soup.find('div', class_=lambda tag: tag and 'article' in tag.lower()) or\n",
    "                  soup.find('div', class_=lambda tag: tag and 'content' in tag.lower()))\n",
    "\n",
    "        if not article:\n",
    "            return None\n",
    "\n",
    "        # Remove unwanted elements\n",
    "        for element in article.find_all(['script', 'style', 'nav', 'footer', 'aside', 'figure', 'img', 'iframe', 'form']):\n",
    "            element.decompose()\n",
    "\n",
    "        # Clean up by removing divs with specific classes (common for ads, share buttons, etc.)\n",
    "        for div in article.find_all('div', class_=lambda x: x and any(cls in x.lower() for cls in ['share', 'related', 'advert', 'comments', 'author'])):\n",
    "            div.decompose()\n",
    "\n",
    "        return article.get_text(separator='\\n', strip=True)\n",
    "\n",
    "    def process_page(self, url, depth):\n",
    "        try:\n",
    "            time.sleep(self.delay)\n",
    "            response = self.session.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            self.urls_visited.add(url)\n",
    "\n",
    "            if self.is_page_an_article(url, soup):\n",
    "                content = self.extract_found_article(soup)\n",
    "                if content:\n",
    "                    # Extract title - specific to Hong Kong Business\n",
    "                    title = (soup.find('h1', class_='article-title') or\n",
    "                            soup.find('h1') or\n",
    "                            soup.find('title'))\n",
    "                    title = title.get_text(strip=True) if title else \"No title\"\n",
    "\n",
    "                    # Extract date - specific to Hong Kong Business\n",
    "                    date = (soup.find('div', class_='date') or\n",
    "                           soup.find('time') or\n",
    "                           soup.find('meta', property='article:published_time'))\n",
    "                    date = (date.get('datetime') if hasattr(date, 'get') and date.get('datetime')\n",
    "                           else date.get_text(strip=True) if date else \"Unknown\")\n",
    "\n",
    "                    self.articles.append({\n",
    "                        'url': url,\n",
    "                        'title': title,\n",
    "                        'date': date,\n",
    "                        'content': content[:5000] + \"...\" if len(content) > 5000 else content,\n",
    "                        \"depth\": depth\n",
    "                    })\n",
    "                    print(f\"Article found at depth {depth}: {title[:50]}...\")\n",
    "\n",
    "            if depth < self.depth and len(self.articles) < self.no_of_articles:\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    absolute_url = urljoin(self.starting_url, link['href'])\n",
    "                    if self.is_url_valid(absolute_url):\n",
    "                        self.queue.append((absolute_url, depth + 1))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {str(e)}\")\n",
    "\n",
    "    def crawl(self):\n",
    "        print(f\"Crawler started crawling on {self.starting_url} (max depth {self.depth})\")\n",
    "        self.queue.append((self.starting_url, 0))\n",
    "        while self.queue and len(self.articles) < self.no_of_articles:\n",
    "            url, depth = self.queue.popleft()\n",
    "            if url not in self.urls_visited:\n",
    "                self.process_page(url, depth)\n",
    "        print(f\"Crawling found {len(self.articles)} articles\")\n",
    "\n",
    "    def save_Results(self, filename=\"NewsArticles.json\"):\n",
    "        import json\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.articles, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Results saved in {filename}\")\n",
    "\n",
    "    def print_summary(self):\n",
    "        print(\"\\nCollected article summary:\")\n",
    "        for idx, article in enumerate(self.articles, 1):\n",
    "            print(f\"\\n{idx}. {article['title']}\")\n",
    "            print(f\"Depth: {article['depth']} | Date: {article['date']}\")\n",
    "            print(f\"URL: {article['url']}\")\n",
    "            print(f\"Preview: {article['content'][:100]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sites_list = {\n",
    "        'Reuters News': 'https://www.reuters.com/',\n",
    "        'Hong Kong Business': 'https://hongkongbusiness.hk/financial-technology',\n",
    "        'SCMP News': 'https://www.scmp.com/',\n",
    "        'BBC News': 'https://www.bbc.com/news'\n",
    "    }\n",
    "\n",
    "    for idx, (name, url) in enumerate(sites_list.items(), 1):\n",
    "        print(f\"{idx}: {name} ({url})\")\n",
    "\n",
    "    choice = int(input(\"\\nSelect a site to crawl (1-4): \")) - 1\n",
    "    selected_url = list(sites_list.values())[choice]\n",
    "\n",
    "    crawler = NewsCrawler(\n",
    "        starting_url=selected_url,\n",
    "        no_of_articles=10,\n",
    "        depth=3,  # Reduced depth as the site might have complex navigation\n",
    "        delay=1  # Increased delay to be polite\n",
    "    )\n",
    "\n",
    "    crawler.crawl()\n",
    "    crawler.print_summary()\n",
    "    crawler.save_Results()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Reuters News (https://www.reuters.com/)\n",
      "2: Hong Kong Business (https://hongkongbusiness.hk/financial-technology)\n",
      "3: SCMP News (https://www.scmp.com/)\n",
      "4: BBC News (https://www.bbc.com/news)\n",
      "Crawler started crawling on https://hongkongbusiness.hk/financial-technology (max depth 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 159\u001B[39m\n\u001B[32m    150\u001B[39m selected_url = \u001B[38;5;28mlist\u001B[39m(sites_list.values())[choice]\n\u001B[32m    152\u001B[39m crawler = NewsCrawler(\n\u001B[32m    153\u001B[39m     starting_url=selected_url,\n\u001B[32m    154\u001B[39m     no_of_articles=\u001B[32m10\u001B[39m,\n\u001B[32m    155\u001B[39m     depth=\u001B[32m3\u001B[39m,  \u001B[38;5;66;03m# Reduced depth as the site might have complex navigation\u001B[39;00m\n\u001B[32m    156\u001B[39m     delay=\u001B[32m1\u001B[39m  \u001B[38;5;66;03m# Increased delay to be polite\u001B[39;00m\n\u001B[32m    157\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m159\u001B[39m \u001B[43mcrawler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcrawl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    160\u001B[39m crawler.print_summary()\n\u001B[32m    161\u001B[39m crawler.save_Results()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 121\u001B[39m, in \u001B[36mNewsCrawler.crawl\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    119\u001B[39m     url, depth = \u001B[38;5;28mself\u001B[39m.queue.popleft()\n\u001B[32m    120\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m url \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.urls_visited:\n\u001B[32m--> \u001B[39m\u001B[32m121\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mprocess_page\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    122\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCrawling found \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m.articles)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m articles\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 78\u001B[39m, in \u001B[36mNewsCrawler.process_page\u001B[39m\u001B[34m(self, url, depth)\u001B[39m\n\u001B[32m     76\u001B[39m response = \u001B[38;5;28mself\u001B[39m.session.get(url)\n\u001B[32m     77\u001B[39m response.raise_for_status()\n\u001B[32m---> \u001B[39m\u001B[32m78\u001B[39m soup = \u001B[43mBeautifulSoup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhtml.parser\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     79\u001B[39m \u001B[38;5;28mself\u001B[39m.urls_visited.add(url)\n\u001B[32m     81\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.is_page_an_article(url, soup):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\bs4\\__init__.py:473\u001B[39m, in \u001B[36mBeautifulSoup.__init__\u001B[39m\u001B[34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001B[39m\n\u001B[32m    471\u001B[39m \u001B[38;5;28mself\u001B[39m.builder.initialize_soup(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m    472\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m473\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_feed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    474\u001B[39m     success = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    475\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\bs4\\__init__.py:658\u001B[39m, in \u001B[36mBeautifulSoup._feed\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    655\u001B[39m \u001B[38;5;28mself\u001B[39m.builder.reset()\n\u001B[32m    657\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.markup \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m658\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbuilder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfeed\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmarkup\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    659\u001B[39m \u001B[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001B[39;00m\n\u001B[32m    660\u001B[39m \u001B[38;5;28mself\u001B[39m.endData()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:467\u001B[39m, in \u001B[36mHTMLParserTreeBuilder.feed\u001B[39m\u001B[34m(self, markup)\u001B[39m\n\u001B[32m    464\u001B[39m parser = BeautifulSoupHTMLParser(\u001B[38;5;28mself\u001B[39m.soup, *args, **kwargs)\n\u001B[32m    466\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m467\u001B[39m     \u001B[43mparser\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfeed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmarkup\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    468\u001B[39m     parser.close()\n\u001B[32m    469\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    470\u001B[39m     \u001B[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001B[39;00m\n\u001B[32m    471\u001B[39m     \u001B[38;5;66;03m# indicate a fatal problem with the markup, especially\u001B[39;00m\n\u001B[32m    472\u001B[39m     \u001B[38;5;66;03m# when there's an error in the doctype declaration.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\Lib\\html\\parser.py:111\u001B[39m, in \u001B[36mHTMLParser.feed\u001B[39m\u001B[34m(self, data)\u001B[39m\n\u001B[32m    105\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"Feed data to the parser.\u001B[39;00m\n\u001B[32m    106\u001B[39m \n\u001B[32m    107\u001B[39m \u001B[33;03mCall this as often as you want, with as little or as much text\u001B[39;00m\n\u001B[32m    108\u001B[39m \u001B[33;03mas you want (may include '\\n').\u001B[39;00m\n\u001B[32m    109\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    110\u001B[39m \u001B[38;5;28mself\u001B[39m.rawdata = \u001B[38;5;28mself\u001B[39m.rawdata + data\n\u001B[32m--> \u001B[39m\u001B[32m111\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgoahead\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\Lib\\html\\parser.py:171\u001B[39m, in \u001B[36mHTMLParser.goahead\u001B[39m\u001B[34m(self, end)\u001B[39m\n\u001B[32m    169\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m startswith(\u001B[33m'\u001B[39m\u001B[33m<\u001B[39m\u001B[33m'\u001B[39m, i):\n\u001B[32m    170\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m starttagopen.match(rawdata, i): \u001B[38;5;66;03m# < + letter\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m171\u001B[39m         k = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparse_starttag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    172\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m startswith(\u001B[33m\"\u001B[39m\u001B[33m</\u001B[39m\u001B[33m\"\u001B[39m, i):\n\u001B[32m    173\u001B[39m         k = \u001B[38;5;28mself\u001B[39m.parse_endtag(i)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\Lib\\html\\parser.py:328\u001B[39m, in \u001B[36mHTMLParser.parse_starttag\u001B[39m\u001B[34m(self, i)\u001B[39m\n\u001B[32m    326\u001B[39m         attrvalue = unescape(attrvalue)\n\u001B[32m    327\u001B[39m     attrs.append((attrname.lower(), attrvalue))\n\u001B[32m--> \u001B[39m\u001B[32m328\u001B[39m     k = \u001B[43mm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    330\u001B[39m end = rawdata[k:endpos].strip()\n\u001B[32m    331\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m end \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[33m\"\u001B[39m\u001B[33m>\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m/>\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2bfccc5ef2180baa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
